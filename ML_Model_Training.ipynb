{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54c1ed7",
   "metadata": {},
   "source": [
    "# Air Quality Prediction - ML Model Training\n",
    "\n",
    "This notebook trains machine learning models to predict air quality parameters (PM2.5, PM10, NO, NO2, CO, CO2) based on:\n",
    "- **Location features**: OpenStreetMap features around monitoring stations\n",
    "- **Temporal features**: Time-based patterns (hour, day, month, etc.)\n",
    "\n",
    "## Model Strategy\n",
    "- Multi-output regression (predicting multiple pollutants simultaneously)\n",
    "- Easy model switching for experimentation\n",
    "- Designed for continuous training pipeline (Jenkins/Cronjobs)\n",
    "- Modular architecture for future enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee9baba",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b45ce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./.venv/lib/python3.13/site-packages (1.5.3)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (3.0.0)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (26.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.1.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl (8.0 MB)\n",
      "Downloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl (64 kB)\n",
      "Downloading pillow-12.1.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Downloading scipy-1.17.0-cp313-cp313-macosx_14_0_arm64.whl (20.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, scikit-learn, matplotlib, seaborn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [seaborn]0/11\u001b[0m [seaborn]ib]n]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.0 pyparsing-3.3.2 scikit-learn-1.8.0 scipy-1.17.0 seaborn-0.13.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6a6649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 3.0.0\n",
      "NumPy version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f27a8",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set paths and parameters for the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c17d4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Output directory: output\n",
      "  OSM features file: station_osm_features_filtered_110.csv\n",
      "  Model save directory: models\n",
      "  Results directory: results\n",
      "  Target pollutants: ['PM2.5', 'PM10', 'NO', 'NO2', 'CO', 'CO2']\n",
      "  Test size: 0.2\n",
      "  Random state: 42\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "OUTPUT_DIR = 'output'  # Directory containing station chunks\n",
    "OSM_FEATURES_FILE = 'station_osm_features_filtered_110.csv'  # OSM features for 110 stations\n",
    "MODEL_SAVE_DIR = 'models'  # Directory to save trained models\n",
    "RESULTS_DIR = 'results'  # Directory to save results and plots\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Target pollutants to predict\n",
    "TARGET_POLLUTANTS = ['PM2.5', 'PM10', 'NO', 'NO2', 'CO', 'CO2']\n",
    "\n",
    "# Test size for train-test split\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  OSM features file: {OSM_FEATURES_FILE}\")\n",
    "print(f\"  Model save directory: {MODEL_SAVE_DIR}\")\n",
    "print(f\"  Results directory: {RESULTS_DIR}\")\n",
    "print(f\"  Target pollutants: {TARGET_POLLUTANTS}\")\n",
    "print(f\"  Test size: {TEST_SIZE}\")\n",
    "print(f\"  Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286462a",
   "metadata": {},
   "source": [
    "## 3. Load and Combine Air Quality Data\n",
    "\n",
    "Load all chunked CSV files from the output directory and combine them into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed23e0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 178 chunk files\n",
      "\n",
      "Sample files:\n",
      "  1. output/DL015/DL015_chunk_3.csv\n",
      "  2. output/DL015/DL015_chunk_2.csv\n",
      "  3. output/DL015/DL015_chunk_1.csv\n",
      "  4. output/DL012/DL012_chunk_1.csv\n",
      "  5. output/DL012/DL012_chunk_3.csv\n",
      "  ... and 173 more files\n"
     ]
    }
   ],
   "source": [
    "# Find all CSV files in output directory\n",
    "all_chunks = glob.glob(os.path.join(OUTPUT_DIR, '*', '*.csv'))\n",
    "\n",
    "print(f\"Found {len(all_chunks)} chunk files\")\n",
    "print(f\"\\nSample files:\")\n",
    "for i, file in enumerate(all_chunks[:5]):\n",
    "    print(f\"  {i+1}. {file}\")\n",
    "\n",
    "if len(all_chunks) > 5:\n",
    "    print(f\"  ... and {len(all_chunks) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bf77d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all chunk files...\n",
      "  Loaded 10/178 files...\n",
      "  Loaded 20/178 files...\n",
      "  Loaded 30/178 files...\n",
      "  Loaded 40/178 files...\n",
      "  Loaded 50/178 files...\n",
      "  Loaded 60/178 files...\n",
      "  Loaded 70/178 files...\n",
      "  Loaded 80/178 files...\n",
      "  Loaded 90/178 files...\n",
      "  Loaded 100/178 files...\n",
      "  Loaded 110/178 files...\n",
      "  Loaded 120/178 files...\n",
      "  Loaded 130/178 files...\n",
      "  Loaded 140/178 files...\n",
      "  Loaded 150/178 files...\n",
      "  Loaded 160/178 files...\n",
      "  Loaded 170/178 files...\n",
      "\n",
      "Combined dataset:\n",
      "  Total rows: 1,452,503\n",
      "  Total columns: 42\n",
      "  Memory usage: 2573.12 MB\n",
      "\n",
      "Columns: ['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket', 'aeroway', 'amenity', 'barrier', 'boundary', 'building', 'craft', 'emergency', 'healthcare', 'highway', 'historic', 'industrial', 'landuse', 'leisure', 'man_made', 'military', 'natural', 'office', 'place', 'power', 'public_transport', 'railway', 'shop', 'sport', 'tourism', 'water', 'waterway']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>...</th>\n",
       "      <th>office</th>\n",
       "      <th>place</th>\n",
       "      <th>power</th>\n",
       "      <th>public_transport</th>\n",
       "      <th>railway</th>\n",
       "      <th>shop</th>\n",
       "      <th>sport</th>\n",
       "      <th>tourism</th>\n",
       "      <th>water</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 17:00:00</td>\n",
       "      <td>38.5</td>\n",
       "      <td>116.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 18:00:00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>128.00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>27.80</td>\n",
       "      <td>18.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 19:00:00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.90</td>\n",
       "      <td>21.33</td>\n",
       "      <td>14.53</td>\n",
       "      <td>51.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.88</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 20:00:00</td>\n",
       "      <td>69.0</td>\n",
       "      <td>160.00</td>\n",
       "      <td>2.83</td>\n",
       "      <td>19.88</td>\n",
       "      <td>12.85</td>\n",
       "      <td>51.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.35</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 21:00:00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>78.00</td>\n",
       "      <td>4.42</td>\n",
       "      <td>32.88</td>\n",
       "      <td>21.18</td>\n",
       "      <td>51.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId             Datetime  PM2.5    PM10    NO    NO2    NOx    NH3  \\\n",
       "0     DL015  2020-05-14 17:00:00   38.5  116.75   NaN    NaN    NaN    NaN   \n",
       "1     DL015  2020-05-14 18:00:00   36.0  128.00  4.60  27.80  18.60    NaN   \n",
       "2     DL015  2020-05-14 19:00:00   75.0     NaN  3.90  21.33  14.53  51.65   \n",
       "3     DL015  2020-05-14 20:00:00   69.0  160.00  2.83  19.88  12.85  51.42   \n",
       "4     DL015  2020-05-14 21:00:00   51.0   78.00  4.42  32.88  21.18  51.35   \n",
       "\n",
       "   CO   SO2  ...                                        office  \\\n",
       "0 NaN  1.25  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "1 NaN  2.30  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "2 NaN  2.88  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "3 NaN  2.35  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "4 NaN  1.95  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "\n",
       "                          place                        power  \\\n",
       "0  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "1  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "2  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "3  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "4  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "\n",
       "         public_transport                railway  \\\n",
       "0  platform,stop_position  construction,proposed   \n",
       "1  platform,stop_position  construction,proposed   \n",
       "2  platform,stop_position  construction,proposed   \n",
       "3  platform,stop_position  construction,proposed   \n",
       "4  platform,stop_position  construction,proposed   \n",
       "\n",
       "                                                shop sport tourism  water  \\\n",
       "0  appliance,beauty,beverages,clothes,confectione...   NaN     NaN  river   \n",
       "1  appliance,beauty,beverages,clothes,confectione...   NaN     NaN  river   \n",
       "2  appliance,beauty,beverages,clothes,confectione...   NaN     NaN  river   \n",
       "3  appliance,beauty,beverages,clothes,confectione...   NaN     NaN  river   \n",
       "4  appliance,beauty,beverages,clothes,confectione...   NaN     NaN  river   \n",
       "\n",
       "      waterway  \n",
       "0  canal,drain  \n",
       "1  canal,drain  \n",
       "2  canal,drain  \n",
       "3  canal,drain  \n",
       "4  canal,drain  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and combine all chunks\n",
    "print(\"Loading all chunk files...\")\n",
    "df_chunks_list = []\n",
    "\n",
    "for i, chunk_file in enumerate(all_chunks):\n",
    "    try:\n",
    "        df_chunk = pd.read_csv(chunk_file)\n",
    "        df_chunks_list.append(df_chunk)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Loaded {i + 1}/{len(all_chunks)} files...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading {chunk_file}: {e}\")\n",
    "\n",
    "# Combine all chunks\n",
    "df_aq = pd.concat(df_chunks_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\nCombined dataset:\")\n",
    "print(f\"  Total rows: {len(df_aq):,}\")\n",
    "print(f\"  Total columns: {len(df_aq.columns)}\")\n",
    "print(f\"  Memory usage: {df_aq.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"\\nColumns: {df_aq.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_aq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36c933",
   "metadata": {},
   "source": [
    "## 4. Load OSM Features\n",
    "\n",
    "Load the OpenStreetMap features for the 110 stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683fb4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSM Features dataset:\n",
      "  Total stations: 96\n",
      "  Total features: 33\n",
      "\n",
      "Columns: ['station_id', 'original_station_id', 'station_name', 'latitude', 'longitude', '_total_elements', '_unique_feature_types', 'aeroway', 'amenity', 'barrier', 'boundary', 'building', 'craft', 'emergency', 'healthcare', 'highway', 'historic', 'industrial', 'landuse', 'leisure', 'man_made', 'military', 'natural', 'office', 'place', 'power', 'public_transport', 'railway', 'shop', 'sport', 'tourism', 'water', 'waterway']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>original_station_id</th>\n",
       "      <th>station_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>_total_elements</th>\n",
       "      <th>_unique_feature_types</th>\n",
       "      <th>aeroway</th>\n",
       "      <th>amenity</th>\n",
       "      <th>barrier</th>\n",
       "      <th>...</th>\n",
       "      <th>office</th>\n",
       "      <th>place</th>\n",
       "      <th>power</th>\n",
       "      <th>public_transport</th>\n",
       "      <th>railway</th>\n",
       "      <th>shop</th>\n",
       "      <th>sport</th>\n",
       "      <th>tourism</th>\n",
       "      <th>water</th>\n",
       "      <th>waterway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TN005</td>\n",
       "      <td>site_5094</td>\n",
       "      <td>SIDCO Kurichi, Coimbatore - TNPCB</td>\n",
       "      <td>10.942451</td>\n",
       "      <td>76.978996</td>\n",
       "      <td>22953</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>college,fuel</td>\n",
       "      <td>gate</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL034</td>\n",
       "      <td>site_119</td>\n",
       "      <td>Sirifort, Delhi - CPCB</td>\n",
       "      <td>28.550425</td>\n",
       "      <td>77.215938</td>\n",
       "      <td>11277</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bench,cafe,charging_station,clinic,college,con...</td>\n",
       "      <td>bollard,city_wall,cycle_barrier,entrance,fence...</td>\n",
       "      <td>...</td>\n",
       "      <td>company,diplomatic,government,lawyer</td>\n",
       "      <td>neighbourhood,suburb</td>\n",
       "      <td>pole,substation</td>\n",
       "      <td>platform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>art,bakery,beauty,boutique,clothes,electrical,...</td>\n",
       "      <td>basketball,cricket,soccer,squash;badminton,ten...</td>\n",
       "      <td>gallery,museum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL017</td>\n",
       "      <td>site_5395</td>\n",
       "      <td>Lodhi Road, Delhi - IITM</td>\n",
       "      <td>28.588333</td>\n",
       "      <td>77.221667</td>\n",
       "      <td>12472</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arts_centre,atm,bank,bar,bicycle_rental,cafe,c...</td>\n",
       "      <td>fence,gate,kerb,lift_gate,wall</td>\n",
       "      <td>...</td>\n",
       "      <td>association,company,diplomatic,government,ngo</td>\n",
       "      <td>neighbourhood</td>\n",
       "      <td>generator</td>\n",
       "      <td>platform</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bakery,bicycle,books,convenience,dairy,florist</td>\n",
       "      <td>tennis</td>\n",
       "      <td>artwork,attraction</td>\n",
       "      <td>pond</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MH014</td>\n",
       "      <td>site_5115</td>\n",
       "      <td>Worli, Mumbai - MPCB</td>\n",
       "      <td>18.993616</td>\n",
       "      <td>72.812811</td>\n",
       "      <td>312159</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe,crematorium,fast_food,fuel,planetarium,sc...</td>\n",
       "      <td>gate</td>\n",
       "      <td>...</td>\n",
       "      <td>financial,government</td>\n",
       "      <td>island,locality,neighbourhood,sea,state</td>\n",
       "      <td>NaN</td>\n",
       "      <td>platform,station,stop_area,stop_position</td>\n",
       "      <td>station,stop,subway</td>\n",
       "      <td>mall</td>\n",
       "      <td>horse_racing,swimming</td>\n",
       "      <td>attraction,museum</td>\n",
       "      <td>canal</td>\n",
       "      <td>drain,lock_gate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TN004</td>\n",
       "      <td>site_288</td>\n",
       "      <td>Velachery Res. Area, Chennai - CPCB</td>\n",
       "      <td>13.005219</td>\n",
       "      <td>80.239812</td>\n",
       "      <td>12305</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>atm,bench,bicycle_parking,cafe,college,drinkin...</td>\n",
       "      <td>block,gate,hedge,lift_gate,median,wall</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neighbourhood</td>\n",
       "      <td>NaN</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stationery,tea</td>\n",
       "      <td>cricket,racquet,tennis</td>\n",
       "      <td>apartment,artwork,attraction,zoo</td>\n",
       "      <td>lake,pond</td>\n",
       "      <td>stream</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id original_station_id                         station_name  \\\n",
       "0      TN005           site_5094    SIDCO Kurichi, Coimbatore - TNPCB   \n",
       "1      DL034            site_119               Sirifort, Delhi - CPCB   \n",
       "2      DL017           site_5395             Lodhi Road, Delhi - IITM   \n",
       "3      MH014           site_5115                 Worli, Mumbai - MPCB   \n",
       "4      TN004            site_288  Velachery Res. Area, Chennai - CPCB   \n",
       "\n",
       "    latitude  longitude  _total_elements  _unique_feature_types aeroway  \\\n",
       "0  10.942451  76.978996            22953                      6     NaN   \n",
       "1  28.550425  77.215938            11277                     18     NaN   \n",
       "2  28.588333  77.221667            12472                     20     NaN   \n",
       "3  18.993616  72.812811           312159                     18     NaN   \n",
       "4  13.005219  80.239812            12305                     17     NaN   \n",
       "\n",
       "                                             amenity  \\\n",
       "0                                       college,fuel   \n",
       "1  bench,cafe,charging_station,clinic,college,con...   \n",
       "2  arts_centre,atm,bank,bar,bicycle_rental,cafe,c...   \n",
       "3  cafe,crematorium,fast_food,fuel,planetarium,sc...   \n",
       "4  atm,bench,bicycle_parking,cafe,college,drinkin...   \n",
       "\n",
       "                                             barrier  ...  \\\n",
       "0                                               gate  ...   \n",
       "1  bollard,city_wall,cycle_barrier,entrance,fence...  ...   \n",
       "2                     fence,gate,kerb,lift_gate,wall  ...   \n",
       "3                                               gate  ...   \n",
       "4             block,gate,hedge,lift_gate,median,wall  ...   \n",
       "\n",
       "                                          office  \\\n",
       "0                                            NaN   \n",
       "1           company,diplomatic,government,lawyer   \n",
       "2  association,company,diplomatic,government,ngo   \n",
       "3                           financial,government   \n",
       "4                                            NaN   \n",
       "\n",
       "                                     place            power  \\\n",
       "0                                      NaN              NaN   \n",
       "1                     neighbourhood,suburb  pole,substation   \n",
       "2                            neighbourhood        generator   \n",
       "3  island,locality,neighbourhood,sea,state              NaN   \n",
       "4                            neighbourhood              NaN   \n",
       "\n",
       "                           public_transport              railway  \\\n",
       "0                                       NaN                 rail   \n",
       "1                                  platform                  NaN   \n",
       "2                                  platform                  NaN   \n",
       "3  platform,station,stop_area,stop_position  station,stop,subway   \n",
       "4                    platform,stop_position                  NaN   \n",
       "\n",
       "                                                shop  \\\n",
       "0                                                NaN   \n",
       "1  art,bakery,beauty,boutique,clothes,electrical,...   \n",
       "2     bakery,bicycle,books,convenience,dairy,florist   \n",
       "3                                               mall   \n",
       "4                                     stationery,tea   \n",
       "\n",
       "                                               sport  \\\n",
       "0                                                NaN   \n",
       "1  basketball,cricket,soccer,squash;badminton,ten...   \n",
       "2                                             tennis   \n",
       "3                              horse_racing,swimming   \n",
       "4                             cricket,racquet,tennis   \n",
       "\n",
       "                            tourism      water         waterway  \n",
       "0                               NaN        NaN              NaN  \n",
       "1                    gallery,museum        NaN              NaN  \n",
       "2                artwork,attraction       pond              NaN  \n",
       "3                 attraction,museum      canal  drain,lock_gate  \n",
       "4  apartment,artwork,attraction,zoo  lake,pond           stream  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load OSM features\n",
    "df_osm = pd.read_csv(OSM_FEATURES_FILE)\n",
    "\n",
    "print(f\"OSM Features dataset:\")\n",
    "print(f\"  Total stations: {len(df_osm)}\")\n",
    "print(f\"  Total features: {len(df_osm.columns)}\")\n",
    "print(f\"\\nColumns: {df_osm.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_osm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ae069",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing\n",
    "\n",
    "### 5.1 Parse DateTime and Create Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dfb78e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found datetime column: Datetime\n",
      "\n",
      "Temporal features created:\n",
      "  - year, month, day, hour\n",
      "  - day_of_week, day_of_year, week_of_year\n",
      "  - is_weekend\n",
      "  - hour_sin, hour_cos (cyclical encoding)\n",
      "  - month_sin, month_cos (cyclical encoding)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>...</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 17:00:00</td>\n",
       "      <td>38.5</td>\n",
       "      <td>116.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 18:00:00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>128.00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>27.80</td>\n",
       "      <td>18.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 19:00:00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.90</td>\n",
       "      <td>21.33</td>\n",
       "      <td>14.53</td>\n",
       "      <td>51.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.88</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 20:00:00</td>\n",
       "      <td>69.0</td>\n",
       "      <td>160.00</td>\n",
       "      <td>2.83</td>\n",
       "      <td>19.88</td>\n",
       "      <td>12.85</td>\n",
       "      <td>51.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.35</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 21:00:00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>78.00</td>\n",
       "      <td>4.42</td>\n",
       "      <td>32.88</td>\n",
       "      <td>21.18</td>\n",
       "      <td>51.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>135</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId            Datetime  PM2.5    PM10    NO    NO2    NOx    NH3  CO  \\\n",
       "0     DL015 2020-05-14 17:00:00   38.5  116.75   NaN    NaN    NaN    NaN NaN   \n",
       "1     DL015 2020-05-14 18:00:00   36.0  128.00  4.60  27.80  18.60    NaN NaN   \n",
       "2     DL015 2020-05-14 19:00:00   75.0     NaN  3.90  21.33  14.53  51.65 NaN   \n",
       "3     DL015 2020-05-14 20:00:00   69.0  160.00  2.83  19.88  12.85  51.42 NaN   \n",
       "4     DL015 2020-05-14 21:00:00   51.0   78.00  4.42  32.88  21.18  51.35 NaN   \n",
       "\n",
       "    SO2  ...  day  hour  day_of_week  day_of_year  week_of_year is_weekend  \\\n",
       "0  1.25  ...   14    17            3          135            20          0   \n",
       "1  2.30  ...   14    18            3          135            20          0   \n",
       "2  2.88  ...   14    19            3          135            20          0   \n",
       "3  2.35  ...   14    20            3          135            20          0   \n",
       "4  1.95  ...   14    21            3          135            20          0   \n",
       "\n",
       "   hour_sin      hour_cos month_sin month_cos  \n",
       "0 -0.965926 -2.588190e-01       0.5 -0.866025  \n",
       "1 -1.000000 -1.836970e-16       0.5 -0.866025  \n",
       "2 -0.965926  2.588190e-01       0.5 -0.866025  \n",
       "3 -0.866025  5.000000e-01       0.5 -0.866025  \n",
       "4 -0.707107  7.071068e-01       0.5 -0.866025  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse datetime column (assuming column name is 'Datetime' or 'datetime')\n",
    "datetime_col = None\n",
    "for col in df_aq.columns:\n",
    "    if col.lower() in ['datetime', 'date', 'timestamp', 'time']:\n",
    "        datetime_col = col\n",
    "        break\n",
    "\n",
    "if datetime_col:\n",
    "    print(f\"Found datetime column: {datetime_col}\")\n",
    "    df_aq[datetime_col] = pd.to_datetime(df_aq[datetime_col])\n",
    "    \n",
    "    # Create temporal features\n",
    "    df_aq['year'] = df_aq[datetime_col].dt.year\n",
    "    df_aq['month'] = df_aq[datetime_col].dt.month\n",
    "    df_aq['day'] = df_aq[datetime_col].dt.day\n",
    "    df_aq['hour'] = df_aq[datetime_col].dt.hour\n",
    "    df_aq['day_of_week'] = df_aq[datetime_col].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df_aq['day_of_year'] = df_aq[datetime_col].dt.dayofyear\n",
    "    df_aq['week_of_year'] = df_aq[datetime_col].dt.isocalendar().week\n",
    "    df_aq['is_weekend'] = (df_aq['day_of_week'] >= 5).astype(int)  # 1 if weekend, 0 otherwise\n",
    "    \n",
    "    # Cyclical encoding for hour (to capture 23:00 and 00:00 proximity)\n",
    "    df_aq['hour_sin'] = np.sin(2 * np.pi * df_aq['hour'] / 24)\n",
    "    df_aq['hour_cos'] = np.cos(2 * np.pi * df_aq['hour'] / 24)\n",
    "    \n",
    "    # Cyclical encoding for month\n",
    "    df_aq['month_sin'] = np.sin(2 * np.pi * df_aq['month'] / 12)\n",
    "    df_aq['month_cos'] = np.cos(2 * np.pi * df_aq['month'] / 12)\n",
    "    \n",
    "    print(f\"\\nTemporal features created:\")\n",
    "    print(f\"  - year, month, day, hour\")\n",
    "    print(f\"  - day_of_week, day_of_year, week_of_year\")\n",
    "    print(f\"  - is_weekend\")\n",
    "    print(f\"  - hour_sin, hour_cos (cyclical encoding)\")\n",
    "    print(f\"  - month_sin, month_cos (cyclical encoding)\")\n",
    "else:\n",
    "    print(\"Warning: No datetime column found!\")\n",
    "    print(f\"Available columns: {df_aq.columns.tolist()}\")\n",
    "\n",
    "# Display sample with temporal features\n",
    "df_aq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53b3f15",
   "metadata": {},
   "source": [
    "### 5.2 Merge Air Quality Data with OSM Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccfc8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station ID column in air quality data: StationId\n",
      "Unique stations in AQ data: 56\n",
      "Unique stations in OSM data: 93\n",
      "\n",
      "Merged dataset:\n",
      "  Total rows: 1,477,165\n",
      "  Total columns: 87\n",
      "  Stations with data: 56\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>...</th>\n",
       "      <th>office_y</th>\n",
       "      <th>place_y</th>\n",
       "      <th>power_y</th>\n",
       "      <th>public_transport_y</th>\n",
       "      <th>railway_y</th>\n",
       "      <th>shop_y</th>\n",
       "      <th>sport_y</th>\n",
       "      <th>tourism_y</th>\n",
       "      <th>water_y</th>\n",
       "      <th>waterway_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 17:00:00</td>\n",
       "      <td>38.5</td>\n",
       "      <td>116.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 18:00:00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>128.00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>27.80</td>\n",
       "      <td>18.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 19:00:00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.90</td>\n",
       "      <td>21.33</td>\n",
       "      <td>14.53</td>\n",
       "      <td>51.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.88</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 20:00:00</td>\n",
       "      <td>69.0</td>\n",
       "      <td>160.00</td>\n",
       "      <td>2.83</td>\n",
       "      <td>19.88</td>\n",
       "      <td>12.85</td>\n",
       "      <td>51.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.35</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DL015</td>\n",
       "      <td>2020-05-14 21:00:00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>78.00</td>\n",
       "      <td>4.42</td>\n",
       "      <td>32.88</td>\n",
       "      <td>21.18</td>\n",
       "      <td>51.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.95</td>\n",
       "      <td>...</td>\n",
       "      <td>bus_inspector,estate_agent,government,lawyer</td>\n",
       "      <td>neighbourhood,quarter,suburb</td>\n",
       "      <td>line,pole,tower,transformer</td>\n",
       "      <td>platform,stop_position</td>\n",
       "      <td>construction,proposed</td>\n",
       "      <td>appliance,beauty,beverages,clothes,confectione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>river</td>\n",
       "      <td>canal,drain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId            Datetime  PM2.5    PM10    NO    NO2    NOx    NH3  CO  \\\n",
       "0     DL015 2020-05-14 17:00:00   38.5  116.75   NaN    NaN    NaN    NaN NaN   \n",
       "1     DL015 2020-05-14 18:00:00   36.0  128.00  4.60  27.80  18.60    NaN NaN   \n",
       "2     DL015 2020-05-14 19:00:00   75.0     NaN  3.90  21.33  14.53  51.65 NaN   \n",
       "3     DL015 2020-05-14 20:00:00   69.0  160.00  2.83  19.88  12.85  51.42 NaN   \n",
       "4     DL015 2020-05-14 21:00:00   51.0   78.00  4.42  32.88  21.18  51.35 NaN   \n",
       "\n",
       "    SO2  ...                                      office_y  \\\n",
       "0  1.25  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "1  2.30  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "2  2.88  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "3  2.35  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "4  1.95  ...  bus_inspector,estate_agent,government,lawyer   \n",
       "\n",
       "                        place_y                      power_y  \\\n",
       "0  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "1  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "2  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "3  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "4  neighbourhood,quarter,suburb  line,pole,tower,transformer   \n",
       "\n",
       "       public_transport_y              railway_y  \\\n",
       "0  platform,stop_position  construction,proposed   \n",
       "1  platform,stop_position  construction,proposed   \n",
       "2  platform,stop_position  construction,proposed   \n",
       "3  platform,stop_position  construction,proposed   \n",
       "4  platform,stop_position  construction,proposed   \n",
       "\n",
       "                                              shop_y sport_y tourism_y  \\\n",
       "0  appliance,beauty,beverages,clothes,confectione...     NaN       NaN   \n",
       "1  appliance,beauty,beverages,clothes,confectione...     NaN       NaN   \n",
       "2  appliance,beauty,beverages,clothes,confectione...     NaN       NaN   \n",
       "3  appliance,beauty,beverages,clothes,confectione...     NaN       NaN   \n",
       "4  appliance,beauty,beverages,clothes,confectione...     NaN       NaN   \n",
       "\n",
       "  water_y   waterway_y  \n",
       "0   river  canal,drain  \n",
       "1   river  canal,drain  \n",
       "2   river  canal,drain  \n",
       "3   river  canal,drain  \n",
       "4   river  canal,drain  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find station ID column in air quality data\n",
    "station_id_col = None\n",
    "for col in df_aq.columns:\n",
    "    if 'station' in col.lower() and 'id' in col.lower():\n",
    "        station_id_col = col\n",
    "        break\n",
    "\n",
    "if not station_id_col:\n",
    "    # Try just 'StationId' or similar\n",
    "    for col in df_aq.columns:\n",
    "        if col.lower() in ['stationid', 'station_id', 'station']:\n",
    "            station_id_col = col\n",
    "            break\n",
    "\n",
    "print(f\"Station ID column in air quality data: {station_id_col}\")\n",
    "print(f\"Unique stations in AQ data: {df_aq[station_id_col].nunique()}\")\n",
    "print(f\"Unique stations in OSM data: {df_osm['station_id'].nunique()}\")\n",
    "\n",
    "# Merge datasets\n",
    "df_merged = df_aq.merge(\n",
    "    df_osm,\n",
    "    left_on=station_id_col,\n",
    "    right_on='station_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged dataset:\")\n",
    "print(f\"  Total rows: {len(df_merged):,}\")\n",
    "print(f\"  Total columns: {len(df_merged.columns)}\")\n",
    "print(f\"  Stations with data: {df_merged[station_id_col].nunique()}\")\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3431c",
   "metadata": {},
   "source": [
    "### 5.3 Prepare Feature Matrix and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30bb2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns (73):\n",
      "['NOx', 'NH3', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket', 'aeroway_x', 'amenity_x', 'barrier_x', 'boundary_x', 'building_x', 'craft_x', 'emergency_x', 'healthcare_x', 'highway_x', 'historic_x', 'industrial_x', 'landuse_x', 'leisure_x', 'man_made_x', 'military_x', 'natural_x', 'office_x', 'place_x', 'power_x', 'public_transport_x', 'railway_x', 'shop_x', 'sport_x', 'tourism_x', 'water_x', 'waterway_x', 'year', 'month', 'day', 'hour', 'day_of_week', 'day_of_year', 'week_of_year', 'is_weekend', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'aeroway_y', 'amenity_y', 'barrier_y', 'boundary_y', 'building_y', 'craft_y', 'emergency_y', 'healthcare_y', 'highway_y', 'historic_y', 'industrial_y', 'landuse_y', 'leisure_y', 'man_made_y', 'military_y', 'natural_y', 'office_y', 'place_y', 'power_y', 'public_transport_y', 'railway_y', 'shop_y', 'sport_y', 'tourism_y', 'water_y', 'waterway_y']\n",
      "\n",
      "Available target pollutants (5): ['PM2.5', 'PM10', 'NO', 'NO2', 'CO']\n",
      "Missing target pollutants (1): ['CO2']\n"
     ]
    }
   ],
   "source": [
    "# Define columns to exclude from features\n",
    "exclude_cols = [\n",
    "    station_id_col,\n",
    "    'station_id',\n",
    "    'original_station_id',\n",
    "    'station_name',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    datetime_col if datetime_col else 'datetime',\n",
    "    '_total_elements',\n",
    "    '_unique_feature_types'\n",
    "] + TARGET_POLLUTANTS\n",
    "\n",
    "# Get feature columns (all columns except excluded ones and target pollutants)\n",
    "feature_cols = [col for col in df_merged.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Check which target pollutants are available\n",
    "available_targets = [col for col in TARGET_POLLUTANTS if col in df_merged.columns]\n",
    "missing_targets = [col for col in TARGET_POLLUTANTS if col not in df_merged.columns]\n",
    "\n",
    "print(f\"\\nAvailable target pollutants ({len(available_targets)}): {available_targets}\")\n",
    "if missing_targets:\n",
    "    print(f\"Missing target pollutants ({len(missing_targets)}): {missing_targets}\")\n",
    "\n",
    "# Use available targets\n",
    "TARGET_POLLUTANTS = available_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb98aa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix X shape: (1477165, 73)\n",
      "Target matrix y shape: (1477165, 5)\n",
      "\n",
      "Missing values in X:\n",
      "NOx                    270152\n",
      "NH3                    701712\n",
      "SO2                    516056\n",
      "O3                     409212\n",
      "Benzene                554728\n",
      "Toluene                591689\n",
      "Xylene                1181898\n",
      "AQI                    311297\n",
      "AQI_Bucket             311297\n",
      "aeroway_x             1421144\n",
      "amenity_x               60133\n",
      "barrier_x               80951\n",
      "boundary_x             182635\n",
      "building_x             293780\n",
      "craft_x               1259563\n",
      "emergency_x           1361428\n",
      "healthcare_x           703941\n",
      "historic_x            1161778\n",
      "industrial_x          1434893\n",
      "landuse_x               29269\n",
      "leisure_x              116378\n",
      "man_made_x             603705\n",
      "military_x            1477165\n",
      "natural_x              493423\n",
      "office_x               645666\n",
      "place_x                661924\n",
      "power_x                842959\n",
      "public_transport_x     446282\n",
      "railway_x              541046\n",
      "shop_x                 435991\n",
      "sport_x                766889\n",
      "tourism_x              593758\n",
      "water_x                895618\n",
      "waterway_x             763195\n",
      "aeroway_y             1421144\n",
      "amenity_y               60133\n",
      "barrier_y               80951\n",
      "boundary_y             182635\n",
      "building_y             293780\n",
      "craft_y               1259563\n",
      "emergency_y           1361428\n",
      "healthcare_y           703941\n",
      "historic_y            1161778\n",
      "industrial_y          1434893\n",
      "landuse_y               29269\n",
      "leisure_y              116378\n",
      "man_made_y             603705\n",
      "military_y            1477165\n",
      "natural_y              493423\n",
      "office_y               645666\n",
      "place_y                661924\n",
      "power_y                818297\n",
      "public_transport_y     446282\n",
      "railway_y              565708\n",
      "shop_y                 411329\n",
      "sport_y                766889\n",
      "tourism_y              593758\n",
      "water_y                895618\n",
      "waterway_y             787857\n",
      "dtype: int64\n",
      "\n",
      "Missing values in y:\n",
      "PM2.5    380261\n",
      "PM10     500563\n",
      "NO       303612\n",
      "NO2      306098\n",
      "CO       283086\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix X and target matrix y\n",
    "X = df_merged[feature_cols].copy()\n",
    "y = df_merged[TARGET_POLLUTANTS].copy()\n",
    "\n",
    "print(f\"Feature matrix X shape: {X.shape}\")\n",
    "print(f\"Target matrix y shape: {y.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values in X:\")\n",
    "missing_x = X.isnull().sum()\n",
    "print(missing_x[missing_x > 0])\n",
    "\n",
    "print(f\"\\nMissing values in y:\")\n",
    "missing_y = y.isnull().sum()\n",
    "print(missing_y[missing_y > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b51cd",
   "metadata": {},
   "source": [
    "### 5.4 Analyze Missing Data Patterns\n",
    "\n",
    "Before handling missing values, let's understand the missingness patterns in our target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "355931fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Data Analysis for Target Pollutants\n",
      "================================================================================\n",
      "PM2.5     :  380,261 missing (25.74%) | 1,096,904 available\n",
      "PM10      :  500,563 missing (33.89%) |  976,602 available\n",
      "NO        :  303,612 missing (20.55%) | 1,173,553 available\n",
      "NO2       :  306,098 missing (20.72%) | 1,171,067 available\n",
      "CO        :  283,086 missing (19.16%) | 1,194,079 available\n",
      "\n",
      "ALL       :  655,506 rows with ANY missing (44.38%) |  821,659 rows complete (55.62%)\n",
      "AT LEAST ONE: 1,287,399 rows with data (87.15%)\n",
      "\n",
      "Missingness Correlation Matrix:\n",
      "(1.0 = always missing together, 0.0 = independent)\n",
      "       PM2.5  PM10    NO   NO2    CO\n",
      "PM2.5   1.00  0.54  0.69  0.72  0.55\n",
      "PM10    0.54  1.00  0.56  0.58  0.54\n",
      "NO      0.69  0.56  1.00  0.93  0.62\n",
      "NO2     0.72  0.58  0.93  1.00  0.64\n",
      "CO      0.55  0.54  0.62  0.64  1.00\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing data patterns in target variables\n",
    "print(\"Missing Data Analysis for Target Pollutants\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for pollutant in TARGET_POLLUTANTS:\n",
    "    missing_count = y[pollutant].isnull().sum()\n",
    "    missing_pct = (missing_count / len(y)) * 100\n",
    "    available_count = len(y) - missing_count\n",
    "    print(f\"{pollutant:<10}: {missing_count:>8,} missing ({missing_pct:>5.2f}%) | {available_count:>8,} available\")\n",
    "\n",
    "# Check how many rows have ALL pollutants available\n",
    "all_available = (~y.isnull()).all(axis=1).sum()\n",
    "all_available_pct = (all_available / len(y)) * 100\n",
    "print(f\"\\n{'ALL':<10}: {len(y) - all_available:>8,} rows with ANY missing ({100-all_available_pct:>5.2f}%) | {all_available:>8,} rows complete ({all_available_pct:>5.2f}%)\")\n",
    "\n",
    "# Check how many rows have at least ONE pollutant available\n",
    "any_available = (~y.isnull()).any(axis=1).sum()\n",
    "any_available_pct = (any_available / len(y)) * 100\n",
    "print(f\"{'AT LEAST ONE':<10}: {any_available:>8,} rows with data ({any_available_pct:>5.2f}%)\")\n",
    "\n",
    "# Show correlation of missingness\n",
    "print(\"\\nMissingness Correlation Matrix:\")\n",
    "print(\"(1.0 = always missing together, 0.0 = independent)\")\n",
    "missing_corr = y.isnull().corr()\n",
    "print(missing_corr.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9145c9bf",
   "metadata": {},
   "source": [
    "### 5.5 Handle Missing Values and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1e872b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 73\n",
      "Categorical features: 0\n",
      "\n",
      "Before imputation: (1477165, 73)\n",
      "After imputation array shape: (1477165, 72)\n",
      "\n",
      "Feature matrix after imputation: (1477165, 72)\n"
     ]
    }
   ],
   "source": [
    "# Separate numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "if categorical_cols:\n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Handle categorical features (OSM features with comma-separated values)\n",
    "if categorical_cols:\n",
    "    print(\"\\nProcessing categorical features (counting unique values)...\")\n",
    "    for col in categorical_cols:\n",
    "        X[f'{col}_count'] = X[col].fillna('').apply(lambda x: len(str(x).split(',')) if x else 0)\n",
    "        X = X.drop(columns=[col])\n",
    "    \n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"Features after processing: {len(numeric_cols)}\")\n",
    "\n",
    "# Impute missing values - ensure all columns are actually numeric\n",
    "# Force conversion to numeric and drop any columns that can't be converted\n",
    "X_numeric = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "print(f\"\\nBefore imputation: {X_numeric.shape}\")\n",
    "\n",
    "imputer_X = SimpleImputer(strategy='median')\n",
    "X_imputed_array = imputer_X.fit_transform(X_numeric)\n",
    "\n",
    "print(f\"After imputation array shape: {X_imputed_array.shape}\")\n",
    "\n",
    "# Create DataFrame - use actual shape from the imputed array\n",
    "X_imputed = pd.DataFrame(\n",
    "    X_imputed_array,\n",
    "    columns=X_numeric.columns[:X_imputed_array.shape[1]],  # Use only as many column names as we have columns\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "print(f\"\\nFeature matrix after imputation: {X_imputed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c65966",
   "metadata": {},
   "source": [
    "### 5.6 Handle Missing Target Values - Strategy Selection\n",
    "\n",
    "**Choose your strategy** by setting the `MISSING_TARGET_STRATEGY` variable:\n",
    "\n",
    "**Strategy Options:**\n",
    "1. **'impute'** - Fill missing values with forward/backward fill then median (RECOMMENDED - keeps all data)\n",
    "2. **'drop_all'** - Drop rows with ANY missing targets (most data loss, cleanest dataset)\n",
    "3. **'drop_majority'** - Drop rows missing MORE THAN 50% of targets (balanced approach)\n",
    "\n",
    "**Trade-offs:**\n",
    "- Imputation: Keeps all data but introduces estimation error\n",
    "- Drop all: Cleanest but loses ~50% of data\n",
    "- Drop majority: Keeps rows with at least some real measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdeed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected strategy: impute\n",
      "================================================================================\n",
      "Strategy: IMPUTE missing target values\n",
      "  - Forward fill (use previous hour's value)\n",
      "  - Backward fill (use next hour's value)\n",
      "  - Median fill (for remaining NaNs)\n",
      "  - Keeps ALL data - no rows dropped!\n",
      "\n",
      "Result:\n",
      "  Rows kept: 1,477,165 (100.0%)\n",
      "  Rows dropped: 0\n",
      "\n",
      "Final dataset:\n",
      "  X shape: (1477165, 72)\n",
      "  y shape: (1477165, 5)\n",
      "  Missing values in y: 310791\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHANGE THIS VARIABLE TO SELECT MISSING DATA STRATEGY\n",
    "# ============================================================================\n",
    "MISSING_TARGET_STRATEGY = 'impute'  # Options: 'impute', 'drop_all', 'drop_majority'\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Selected strategy: {MISSING_TARGET_STRATEGY}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if MISSING_TARGET_STRATEGY == 'impute':\n",
    "    print(\"Strategy: IMPUTE missing target values\")\n",
    "    print(\"  - Forward fill (use previous hour's value)\")\n",
    "    print(\"  - Backward fill (use next hour's value)\")\n",
    "    print(\"  - Median fill (for remaining NaNs)\")\n",
    "    print(\"  - Keeps ALL data - no rows dropped!\")\n",
    "    \n",
    "    # Sort by datetime to ensure proper forward/backward fill\n",
    "    if datetime_col and datetime_col in df_merged.columns:\n",
    "        # Get the original order indices\n",
    "        original_index = y.index\n",
    "        \n",
    "        # Create a temporary dataframe with datetime for proper sorting\n",
    "        y_with_time = y.copy()\n",
    "        y_with_time['_datetime'] = df_merged.loc[y.index, datetime_col]\n",
    "        y_with_time['_station'] = df_merged.loc[y.index, station_id_col]\n",
    "        \n",
    "        # Sort by station and time\n",
    "        y_sorted = y_with_time.sort_values(['_station', '_datetime'])\n",
    "        \n",
    "        # Apply imputation separately for each station\n",
    "        y_imputed_list = []\n",
    "        for station in y_sorted['_station'].unique():\n",
    "            station_mask = y_sorted['_station'] == station\n",
    "            y_station = y_sorted[station_mask][TARGET_POLLUTANTS].copy()\n",
    "            \n",
    "            # Forward fill, then backward fill, then median\n",
    "            y_station = y_station.ffill().bfill()\n",
    "            \n",
    "            # Fill any remaining NaNs with median\n",
    "            for col in TARGET_POLLUTANTS:\n",
    "                if y_station[col].isnull().any():\n",
    "                    median_val = y_station[col].median()\n",
    "                    if pd.isna(median_val):  # If still NaN, use global median\n",
    "                        median_val = y[col].median()\n",
    "                    y_station[col].fillna(median_val, inplace=True)\n",
    "            \n",
    "            y_imputed_list.append(y_station)\n",
    "        \n",
    "        # Combine and restore original order\n",
    "        y_imputed_sorted = pd.concat(y_imputed_list)\n",
    "        y_imputed = y_imputed_sorted.reindex(original_index)\n",
    "    else:\n",
    "        # Simple imputation without time-based ordering\n",
    "        y_imputed = y.copy()\n",
    "        for col in TARGET_POLLUTANTS:\n",
    "            median_val = y[col].median()\n",
    "            y_imputed[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    X_clean = X_imputed.copy()\n",
    "    y_clean = y_imputed.copy()\n",
    "    \n",
    "    print(f\"\\nResult:\")\n",
    "    print(f\"  Rows kept: {len(X_clean):,} (100.0%)\")\n",
    "    print(f\"  Rows dropped: 0\")\n",
    "    \n",
    "elif MISSING_TARGET_STRATEGY == 'drop_all':\n",
    "    print(\"Strategy: DROP rows with ANY missing target values\")\n",
    "    print(\"  - Most conservative approach\")\n",
    "    print(\"  - Highest data loss but cleanest dataset\")\n",
    "    print(\"  - Only keeps rows with ALL pollutants measured\")\n",
    "    \n",
    "    mask = ~y.isnull().any(axis=1)\n",
    "    X_clean = X_imputed[mask].copy()\n",
    "    y_clean = y[mask].copy()\n",
    "    \n",
    "    rows_kept = len(X_clean)\n",
    "    rows_dropped = len(X_imputed) - rows_kept\n",
    "    pct_kept = (rows_kept / len(X_imputed)) * 100\n",
    "    \n",
    "    print(f\"\\nResult:\")\n",
    "    print(f\"  Rows kept: {rows_kept:,} ({pct_kept:.2f}%)\")\n",
    "    print(f\"  Rows dropped: {rows_dropped:,} ({100-pct_kept:.2f}%)\")\n",
    "    \n",
    "elif MISSING_TARGET_STRATEGY == 'drop_majority':\n",
    "    print(\"Strategy: DROP rows missing MORE THAN 50% of targets\")\n",
    "    print(\"  - Balanced approach\")\n",
    "    print(\"  - Keeps rows with at least 3 out of 6 pollutants\")\n",
    "    print(\"  - Moderate data retention\")\n",
    "    \n",
    "    # Count non-null values per row\n",
    "    non_null_counts = y.notna().sum(axis=1)\n",
    "    threshold = len(TARGET_POLLUTANTS) / 2\n",
    "    \n",
    "    mask = non_null_counts > threshold\n",
    "    X_clean = X_imputed[mask].copy()\n",
    "    y_clean = y[mask].copy()\n",
    "    \n",
    "    # Impute remaining NaNs with median\n",
    "    for col in TARGET_POLLUTANTS:\n",
    "        if y_clean[col].isnull().any():\n",
    "            median_val = y_clean[col].median()\n",
    "            y_clean[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    rows_kept = len(X_clean)\n",
    "    rows_dropped = len(X_imputed) - rows_kept\n",
    "    pct_kept = (rows_kept / len(X_imputed)) * 100\n",
    "    \n",
    "    print(f\"\\nResult:\")\n",
    "    print(f\"  Rows kept: {rows_kept:,} ({pct_kept:.2f}%)\")\n",
    "    print(f\"  Rows dropped: {rows_dropped:,} ({100-pct_kept:.2f}%)\")\n",
    "    print(f\"  Remaining NaNs imputed with median\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Invalid strategy: {MISSING_TARGET_STRATEGY}. Choose 'impute', 'drop_all', or 'drop_majority'\")\n",
    "\n",
    "print(f\"\\nFinal dataset:\")\n",
    "print(f\"  X shape: {X_clean.shape}\")\n",
    "print(f\"  y shape: {y_clean.shape}\")\n",
    "print(f\"  Missing values in y: {y_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4e5d2",
   "metadata": {},
   "source": [
    "### 5.7 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eacc4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split:\n",
      "  Training set: 1,181,732 samples\n",
      "  Testing set: 295,433 samples\n",
      "  Features: 72\n",
      "  Targets: 5\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean,\n",
    "    y_clean,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Testing set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "print(f\"  Targets: {y_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19161de",
   "metadata": {},
   "source": [
    "### 5.8 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56fdc0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled using StandardScaler\n",
      "  Mean: -0.000000\n",
      "  Std: 0.993031\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"  Std: {X_train_scaled.std():.6f}\")\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3cb58",
   "metadata": {},
   "source": [
    "## 6. Model Definition and Selection\n",
    "\n",
    "Define multiple ML models and create a mechanism for easy switching between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90aa2214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      "================================================================================\n",
      "1. linear_regression\n",
      "   Linear Regression - Simple baseline model\n",
      "   Uses scaling: True\n",
      "\n",
      "2. ridge\n",
      "   Ridge Regression - L2 regularization\n",
      "   Uses scaling: True\n",
      "\n",
      "3. lasso\n",
      "   Lasso Regression - L1 regularization with feature selection\n",
      "   Uses scaling: True\n",
      "\n",
      "4. decision_tree\n",
      "   Decision Tree - Non-linear, interpretable\n",
      "   Uses scaling: False\n",
      "\n",
      "5. random_forest\n",
      "   Random Forest - Ensemble of decision trees, robust to overfitting\n",
      "   Uses scaling: False\n",
      "\n",
      "6. gradient_boosting\n",
      "   Gradient Boosting - Sequential ensemble, high accuracy\n",
      "   Uses scaling: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define available models\n",
    "MODELS = {\n",
    "    'linear_regression': {\n",
    "        'model': MultiOutputRegressor(LinearRegression()),\n",
    "        'description': 'Linear Regression - Simple baseline model',\n",
    "        'use_scaling': True\n",
    "    },\n",
    "    'ridge': {\n",
    "        'model': MultiOutputRegressor(Ridge(alpha=1.0, random_state=RANDOM_STATE)),\n",
    "        'description': 'Ridge Regression - L2 regularization',\n",
    "        'use_scaling': True\n",
    "    },\n",
    "    'lasso': {\n",
    "        'model': MultiOutputRegressor(Lasso(alpha=1.0, random_state=RANDOM_STATE)),\n",
    "        'description': 'Lasso Regression - L1 regularization with feature selection',\n",
    "        'use_scaling': True\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': MultiOutputRegressor(DecisionTreeRegressor(random_state=RANDOM_STATE)),\n",
    "        'description': 'Decision Tree - Non-linear, interpretable',\n",
    "        'use_scaling': False\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': MultiOutputRegressor(\n",
    "            RandomForestRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        ),\n",
    "        'description': 'Random Forest - Ensemble of decision trees, robust to overfitting',\n",
    "        'use_scaling': False\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': MultiOutputRegressor(\n",
    "            GradientBoostingRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.1,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        ),\n",
    "        'description': 'Gradient Boosting - Sequential ensemble, high accuracy',\n",
    "        'use_scaling': False\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Available Models:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (model_name, model_info) in enumerate(MODELS.items(), 1):\n",
    "    print(f\"{i}. {model_name}\")\n",
    "    print(f\"   {model_info['description']}\")\n",
    "    print(f\"   Uses scaling: {model_info['use_scaling']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd28b224",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1c8c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def train_model(model_name, model_config, X_train_raw, X_train_sc, X_test_raw, X_test_sc, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a model and evaluate its performance.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model_config: Model configuration dictionary\n",
    "        X_train_raw: Unscaled training features\n",
    "        X_train_sc: Scaled training features\n",
    "        X_test_raw: Unscaled test features\n",
    "        X_test_sc: Scaled test features\n",
    "        y_train: Training targets\n",
    "        y_test: Testing targets\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with trained model and metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining: {model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Description: {model_config['description']}\")\n",
    "    \n",
    "    # Select appropriate data (scaled or unscaled)\n",
    "    X_tr = X_train_sc if model_config['use_scaling'] else X_train_raw\n",
    "    X_te = X_test_sc if model_config['use_scaling'] else X_test_raw\n",
    "    \n",
    "    # Train model\n",
    "    start_time = datetime.now()\n",
    "    model = model_config['model']\n",
    "    model.fit(X_tr, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_tr)\n",
    "    y_test_pred = model.predict(X_te)\n",
    "    \n",
    "    # Calculate metrics for each pollutant\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'model': model,\n",
    "        'training_time': training_time,\n",
    "        'use_scaling': model_config['use_scaling'],\n",
    "        'metrics': {}\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Pollutant':<10} {'MAE':<12} {'RMSE':<12} {'R²':<12} {'Train R²':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, pollutant in enumerate(TARGET_POLLUTANTS):\n",
    "        # Test metrics\n",
    "        mae = mean_absolute_error(y_test.iloc[:, i], y_test_pred[:, i])\n",
    "        rmse = np.sqrt(mean_squared_error(y_test.iloc[:, i], y_test_pred[:, i]))\n",
    "        r2 = r2_score(y_test.iloc[:, i], y_test_pred[:, i])\n",
    "        \n",
    "        # Train R² (to check overfitting)\n",
    "        r2_train = r2_score(y_train.iloc[:, i], y_train_pred[:, i])\n",
    "        \n",
    "        results['metrics'][pollutant] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse,\n",
    "            'R2': r2,\n",
    "            'R2_train': r2_train\n",
    "        }\n",
    "        \n",
    "        print(f\"{pollutant:<10} {mae:<12.4f} {rmse:<12.4f} {r2:<12.4f} {r2_train:<12.4f}\")\n",
    "    \n",
    "    # Overall metrics (average across all pollutants)\n",
    "    avg_mae = np.mean([m['MAE'] for m in results['metrics'].values()])\n",
    "    avg_rmse = np.mean([m['RMSE'] for m in results['metrics'].values()])\n",
    "    avg_r2 = np.mean([m['R2'] for m in results['metrics'].values()])\n",
    "    avg_r2_train = np.mean([m['R2_train'] for m in results['metrics'].values()])\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'AVERAGE':<10} {avg_mae:<12.4f} {avg_rmse:<12.4f} {avg_r2:<12.4f} {avg_r2_train:<12.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results['metrics']['AVERAGE'] = {\n",
    "        'MAE': avg_mae,\n",
    "        'RMSE': avg_rmse,\n",
    "        'R2': avg_r2,\n",
    "        'R2_train': avg_r2_train\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Training function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbb38319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training data validation:\n",
      "================================================================================\n",
      "\n",
      "X_train shape: (1181732, 72)\n",
      "X_test shape: (295433, 72)\n",
      "y_train shape: (1181732, 5)\n",
      "y_test shape: (295433, 5)\n",
      "\n",
      "NaNs in X_train: 0\n",
      "NaNs in X_test: 0\n",
      "NaNs in y_train: 248773\n",
      "NaNs in y_test: 62018\n",
      "\n",
      "⚠️  WARNING: NaN values found in target variables!\n",
      "Applying emergency median imputation...\n",
      "  Filled PM2.5 in y_train with median: 66.50\n",
      "  Filled PM10 in y_train with median: 128.66\n",
      "  Filled PM2.5 in y_test with median: 66.50\n",
      "  Filled PM10 in y_test with median: 129.25\n",
      "\n",
      "✓ Emergency imputation complete\n",
      "Final NaNs in y_train: 0\n",
      "Final NaNs in y_test: 0\n",
      "================================================================================\n",
      "Ready for training!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify no NaN values remain before training\n",
    "print(\"Pre-training data validation:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check for NaN in X\n",
    "X_train_nans = X_train.isnull().sum().sum()\n",
    "X_test_nans = X_test.isnull().sum().sum()\n",
    "print(f\"\\nNaNs in X_train: {X_train_nans}\")\n",
    "print(f\"NaNs in X_test: {X_test_nans}\")\n",
    "\n",
    "# Check for NaN in y\n",
    "y_train_nans = y_train.isnull().sum().sum()\n",
    "y_test_nans = y_test.isnull().sum().sum()\n",
    "print(f\"NaNs in y_train: {y_train_nans}\")\n",
    "print(f\"NaNs in y_test: {y_test_nans}\")\n",
    "\n",
    "# If NaNs found in y, apply emergency fix\n",
    "if y_train_nans > 0 or y_test_nans > 0:\n",
    "    print(\"\\n⚠️  WARNING: NaN values found in target variables!\")\n",
    "    print(\"Applying emergency median imputation...\")\n",
    "    \n",
    "    # Create copies and fill\n",
    "    y_train = y_train.copy()\n",
    "    y_test = y_test.copy()\n",
    "    \n",
    "    for col in y_train.columns:\n",
    "        if y_train[col].isnull().any():\n",
    "            median_val = y_train[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0  # Fallback if entire column is NaN\n",
    "            y_train[col] = y_train[col].fillna(median_val)\n",
    "            print(f\"  Filled {col} in y_train with median: {median_val:.2f}\")\n",
    "    \n",
    "    for col in y_test.columns:\n",
    "        if y_test[col].isnull().any():\n",
    "            median_val = y_test[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = y_train[col].median()  # Use train median\n",
    "            y_test[col] = y_test[col].fillna(median_val)\n",
    "            print(f\"  Filled {col} in y_test with median: {median_val:.2f}\")\n",
    "    \n",
    "    print(\"\\n✓ Emergency imputation complete\")\n",
    "    print(f\"Final NaNs in y_train: {y_train.isnull().sum().sum()}\")\n",
    "    print(f\"Final NaNs in y_test: {y_test.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"\\n✓ All data validated - no NaN values found\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Ready for training!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e7aa9",
   "metadata": {},
   "source": [
    "## 8. Model Training - Manual Selection\n",
    "\n",
    "**IMPORTANT**: Change the `SELECTED_MODEL` variable below to train different models.\n",
    "\n",
    "Available options:\n",
    "- `'linear_regression'`\n",
    "- `'ridge'`\n",
    "- `'lasso'`\n",
    "- `'decision_tree'`\n",
    "- `'random_forest'`\n",
    "- `'gradient_boosting'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b39018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training: random_forest\n",
      "================================================================================\n",
      "Description: Random Forest - Ensemble of decision trees, robust to overfitting\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CHANGE THIS VARIABLE TO SELECT A DIFFERENT MODEL\n",
    "# ============================================================================\n",
    "SELECTED_MODEL = 'random_forest'  # <-- Change this to switch models\n",
    "# ============================================================================\n",
    "\n",
    "if SELECTED_MODEL not in MODELS:\n",
    "    print(f\"Error: '{SELECTED_MODEL}' is not a valid model name.\")\n",
    "    print(f\"Available models: {list(MODELS.keys())}\")\n",
    "else:\n",
    "    # Train the selected model\n",
    "    results = train_model(\n",
    "        SELECTED_MODEL,\n",
    "        MODELS[SELECTED_MODEL],\n",
    "        X_train,\n",
    "        X_train_scaled,\n",
    "        X_test,\n",
    "        X_test_scaled,\n",
    "        y_train,\n",
    "        y_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d06fc9",
   "metadata": {},
   "source": [
    "## 9. Save Trained Model\n",
    "\n",
    "Save the trained model, scaler, and metadata for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for versioning\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f\"{SELECTED_MODEL}_{timestamp}.pkl\"\n",
    "scaler_filename = f\"scaler_{timestamp}.pkl\"\n",
    "metadata_filename = f\"metadata_{timestamp}.txt\"\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(MODEL_SAVE_DIR, model_filename)\n",
    "joblib.dump(results['model'], model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save scaler (if model uses scaling)\n",
    "if results['use_scaling']:\n",
    "    scaler_path = os.path.join(MODEL_SAVE_DIR, scaler_filename)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(MODEL_SAVE_DIR, metadata_filename)\n",
    "with open(metadata_path, 'w') as f:\n",
    "    f.write(f\"Model: {SELECTED_MODEL}\\n\")\n",
    "    f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "    f.write(f\"Training samples: {len(X_train)}\\n\")\n",
    "    f.write(f\"Test samples: {len(X_test)}\\n\")\n",
    "    f.write(f\"Features: {len(X_train.columns)}\\n\")\n",
    "    f.write(f\"Target pollutants: {', '.join(TARGET_POLLUTANTS)}\\n\")\n",
    "    f.write(f\"Uses scaling: {results['use_scaling']}\\n\")\n",
    "    f.write(f\"Training time: {results['training_time']:.2f} seconds\\n\")\n",
    "    f.write(f\"\\nPerformance Metrics:\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    for pollutant, metrics in results['metrics'].items():\n",
    "        f.write(f\"\\n{pollutant}:\\n\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            f.write(f\"  {metric_name}: {value:.4f}\\n\")\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")\n",
    "print(\"\\nModel artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e9f82",
   "metadata": {},
   "source": [
    "## 10. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af28309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle(f'Model Performance: {SELECTED_MODEL}', fontsize=16, fontweight='bold')\n",
    "\n",
    "pollutants = [p for p in TARGET_POLLUTANTS]\n",
    "mae_values = [results['metrics'][p]['MAE'] for p in pollutants]\n",
    "rmse_values = [results['metrics'][p]['RMSE'] for p in pollutants]\n",
    "r2_values = [results['metrics'][p]['R2'] for p in pollutants]\n",
    "\n",
    "# MAE plot\n",
    "axes[0].bar(pollutants, mae_values, color='steelblue')\n",
    "axes[0].set_title('Mean Absolute Error (MAE)', fontweight='bold')\n",
    "axes[0].set_xlabel('Pollutant')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# RMSE plot\n",
    "axes[1].bar(pollutants, rmse_values, color='coral')\n",
    "axes[1].set_title('Root Mean Squared Error (RMSE)', fontweight='bold')\n",
    "axes[1].set_xlabel('Pollutant')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# R² plot\n",
    "axes[2].bar(pollutants, r2_values, color='mediumseagreen')\n",
    "axes[2].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
    "axes[2].set_title('R² Score', fontweight='bold')\n",
    "axes[2].set_xlabel('Pollutant')\n",
    "axes[2].set_ylabel('R²')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, f'{SELECTED_MODEL}_{timestamp}_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to: {os.path.join(RESULTS_DIR, f'{SELECTED_MODEL}_{timestamp}_metrics.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975c056",
   "metadata": {},
   "source": [
    "## 11. Compare All Models (Optional)\n",
    "\n",
    "Run this section to train and compare all available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351a6eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODELS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TRAIN_ALL_MODELS:\n\u001b[32m      5\u001b[39m     all_results = {}\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_config \u001b[38;5;129;01min\u001b[39;00m \u001b[43mMODELS\u001b[49m.items():\n\u001b[32m      8\u001b[39m         result = train_model(\n\u001b[32m      9\u001b[39m             model_name,\n\u001b[32m     10\u001b[39m             model_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m             y_test\n\u001b[32m     17\u001b[39m         )\n\u001b[32m     18\u001b[39m         all_results[model_name] = result\n",
      "\u001b[31mNameError\u001b[39m: name 'MODELS' is not defined"
     ]
    }
   ],
   "source": [
    "# Set to True to train all models\n",
    "TRAIN_ALL_MODELS = True  # <-- Change to True to compare all models\n",
    "\n",
    "if TRAIN_ALL_MODELS:\n",
    "    all_results = {}\n",
    "    \n",
    "    for model_name, model_config in MODELS.items():\n",
    "        result = train_model(\n",
    "            model_name,\n",
    "            model_config,\n",
    "            X_train,\n",
    "            X_train_scaled,\n",
    "            X_test,\n",
    "            X_test_scaled,\n",
    "            y_train,\n",
    "            y_test\n",
    "        )\n",
    "        all_results[model_name] = result\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for model_name, result in all_results.items():\n",
    "        avg_metrics = result['metrics']['AVERAGE']\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'MAE': avg_metrics['MAE'],\n",
    "            'RMSE': avg_metrics['RMSE'],\n",
    "            'R²': avg_metrics['R2'],\n",
    "            'Training Time (s)': result['training_time']\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    df_comparison = df_comparison.sort_values('R²', ascending=False)\n",
    "    \n",
    "    print(\"\\nModel Comparison (sorted by R²):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Save comparison to CSV\n",
    "    comparison_path = os.path.join(RESULTS_DIR, f'model_comparison_{timestamp}.csv')\n",
    "    df_comparison.to_csv(comparison_path, index=False)\n",
    "    print(f\"\\nComparison saved to: {comparison_path}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(df_comparison))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, df_comparison['MAE'], width, label='MAE', color='steelblue')\n",
    "    ax.bar(x, df_comparison['RMSE'], width, label='RMSE', color='coral')\n",
    "    ax.bar(x + width, df_comparison['R²'] * 100, width, label='R² (×100)', color='mediumseagreen')\n",
    "    \n",
    "    ax.set_xlabel('Model', fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontweight='bold')\n",
    "    ax.set_title('Model Comparison - Average Metrics', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_comparison['Model'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, f'model_comparison_{timestamp}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Set TRAIN_ALL_MODELS = True to compare all models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57d825",
   "metadata": {},
   "source": [
    "## 12. Next Steps and Notes\n",
    "\n",
    "### For Continuous Training (CI/CD Pipeline):\n",
    "\n",
    "1. **Jenkins/Cronjob Setup**:\n",
    "   - Schedule this notebook to run periodically (e.g., weekly, monthly)\n",
    "   - Use `papermill` to execute notebook programmatically:\n",
    "     ```bash\n",
    "     papermill ML_Model_Training.ipynb output_notebook.ipynb \\\n",
    "       -p SELECTED_MODEL 'random_forest' \\\n",
    "       -p TEST_SIZE 0.2\n",
    "     ```\n",
    "\n",
    "2. **Model Versioning**:\n",
    "   - Models are saved with timestamps\n",
    "   - Keep track of best performing models\n",
    "   - Implement model registry for production deployment\n",
    "\n",
    "3. **Data Updates**:\n",
    "   - As new data chunks are added to `output/` folder, re-run this notebook\n",
    "   - Models will automatically train on updated dataset\n",
    "\n",
    "4. **Future Enhancements**:\n",
    "   - Add temperature data when available\n",
    "   - Implement hyperparameter tuning (GridSearchCV, RandomizedSearchCV)\n",
    "   - Add cross-validation for more robust evaluation\n",
    "   - Implement advanced models (XGBoost, LightGBM, Neural Networks)\n",
    "   - Add feature importance analysis\n",
    "   - Implement A/B testing for model comparison in production\n",
    "\n",
    "5. **Model Selection Tips**:\n",
    "   - Start with `random_forest` for baseline\n",
    "   - Use `gradient_boosting` for potentially better accuracy\n",
    "   - Try `linear_regression` or `ridge` for interpretability\n",
    "   - Consider ensemble methods (combining multiple models)\n",
    "\n",
    "### Current Limitations:\n",
    "- No temperature data (add when available)\n",
    "- Basic feature engineering (can be enhanced)\n",
    "- No hyperparameter tuning (using default/reasonable values)\n",
    "- No cross-validation (single train-test split)\n",
    "\n",
    "### Recommended Next Actions:\n",
    "1. Run this notebook with different models\n",
    "2. Compare results using the \"Compare All Models\" section\n",
    "3. Select best performing model based on R² and domain knowledge\n",
    "4. Set up automated retraining schedule\n",
    "5. Monitor model performance over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
