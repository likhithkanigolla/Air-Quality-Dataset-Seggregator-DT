{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29504ea",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "Before running the notebook, ensure that the required libraries are installed. You can install them using the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da2839",
   "metadata": {},
   "source": [
    "# OSM Features Extraction for Air Quality Data\n",
    "This notebook extracts OpenStreetMap features based on latitude and longitude for air quality data labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d869d3",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Import libraries such as pandas, requests, and any mapping libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e95b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (3.0.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: geopy in ./.venv/lib/python3.13/site-packages (2.4.1)\n",
      "Requirement already satisfied: overpy in ./.venv/lib/python3.13/site-packages (0.7)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.13/site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in ./.venv/lib/python3.13/site-packages (from geopy) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Library Installation\n",
    "!pip install pandas requests geopy overpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b57d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from geopy.geocoders import Nominatim\n",
    "import overpy\n",
    "\n",
    "# Initialize Overpass API\n",
    "api = overpy.Overpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb5e1c",
   "metadata": {},
   "source": [
    "## Load Station Locations from CSV\n",
    "Load the station locations from the provided CSV file using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e80925b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      station_name station_id   latitude  \\\n",
      "0                SIDCO Kurichi, Coimbatore - TNPCB  site_5094  10.942451   \n",
      "1                    Urban, Chamarajanagar - KSPCB  site_5124  11.553580   \n",
      "2                    MD University, Rohtak - HSPCB   site_147  28.521230   \n",
      "3  IESD Banaras Hindu University, Varanasi - UPPCB  site_5468  25.262326   \n",
      "4                           Sirifort, Delhi - CPCB   site_119  28.550425   \n",
      "\n",
      "   longitude  city  state  \n",
      "0  76.978996   NaN    NaN  \n",
      "1  76.555210   NaN    NaN  \n",
      "2  76.371380   NaN    NaN  \n",
      "3  82.995408   NaN    NaN  \n",
      "4  77.215938   NaN    NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load Station Locations from CSV\n",
    "df_stations = pd.read_csv('station_locations.csv')\n",
    "print(df_stations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7715fc",
   "metadata": {},
   "source": [
    "## Define Function to Fetch OSM Features\n",
    "Create a function that takes latitude and longitude as input and fetches relevant OSM features using an API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3254c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Fetch OSM Features\n",
    "\n",
    "def fetch_osm_features(lat, lon):\n",
    "    # Define your Overpass API query here\n",
    "    query = f'[out:json];(node(around:500,{lat},{lon}););out;'\n",
    "    result = api.query(query)\n",
    "    relevant_features = []\n",
    "    for node in result.nodes:\n",
    "        features = node.tags\n",
    "        # Check for relevant features\n",
    "        if any(feature in features for feature in ['aerialway', 'amenity', 'building', 'highway', 'landuse', 'natural', 'shop']):\n",
    "            relevant_features.append(features)\n",
    "    return relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8d51c",
   "metadata": {},
   "source": [
    "## Map Latitude and Longitude to OSM Features\n",
    "Iterate through the station locations and use the function to map each location to its corresponding OSM features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1fa5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Overpass API for station: SIDCO Kurichi, Coimbatore - TNPCB\n",
      "Location: 10.942451, 76.978996\n",
      "Bounding box: 10.937451,76.973996,10.947451000000001,76.98399599999999\n",
      "\n",
      "Response status code: 200\n",
      "Response content type: application/json\n",
      "Response preview (first 500 chars):\n",
      "{\n",
      "  \"version\": 0.6,\n",
      "  \"generator\": \"Overpass API 0.7.62.10 2d4cfc48\",\n",
      "  \"osm3s\": {\n",
      "    \"timestamp_osm_base\": \"2026-01-29T06:18:10Z\",\n",
      "    \"copyright\": \"The data included in this document is from www.openstreetmap.org. The data is made available under ODbL.\"\n",
      "  },\n",
      "  \"elements\": [\n",
      "\n",
      "{\n",
      "  \"type\": \"node\",\n",
      "  \"id\": 266585747,\n",
      "  \"lat\": 10.9391913,\n",
      "  \"lon\": 76.9810669\n",
      "},\n",
      "{\n",
      "  \"type\": \"node\",\n",
      "  \"id\": 266585748,\n",
      "  \"lat\": 10.9392998,\n",
      "  \"lon\": 76.9799146\n",
      "},\n",
      "{\n",
      "  \"type\": \"node\",\n",
      "  \"id\": 1423799063,\n",
      "  \"lat\": 10.937\n",
      "\n",
      "✓ Successfully received data!\n",
      "Number of elements: 22953\n",
      "\n",
      "First 3 elements:\n",
      "\n",
      "Element 1:\n",
      "  Type: node\n",
      "  ID: 266585747\n",
      "  Tags: {}\n",
      "\n",
      "Element 2:\n",
      "  Type: node\n",
      "  ID: 266585748\n",
      "  Tags: {}\n",
      "\n",
      "Element 3:\n",
      "  Type: node\n",
      "  ID: 1423799063\n",
      "  Tags: {}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Example: bounding box around the first station (small area, ~500m buffer)\n",
    "lat = float(df_stations.iloc[0]['latitude'])\n",
    "lon = float(df_stations.iloc[0]['longitude'])\n",
    "delta = 0.005  # ~500m in degrees\n",
    "\n",
    "bbox = f\"{lat-delta},{lon-delta},{lat+delta},{lon+delta}\"\n",
    "\n",
    "query = f\"\"\"\n",
    "[bbox:{bbox}]\n",
    "[out:json]\n",
    "[timeout:90];\n",
    "(\n",
    "  node({lat-delta},{lon-delta},{lat+delta},{lon+delta});\n",
    "  way({lat-delta},{lon-delta},{lat+delta},{lon+delta});\n",
    "  relation({lat-delta},{lon-delta},{lat+delta},{lon+delta});\n",
    ");\n",
    "out body;\n",
    ">;\n",
    "out skel qt;\n",
    "\"\"\"\n",
    "\n",
    "url = \"https://overpass-api.de/api/interpreter\"\n",
    "\n",
    "print(f\"Querying Overpass API for station: {df_stations.iloc[0]['station_name']}\")\n",
    "print(f\"Location: {lat}, {lon}\")\n",
    "print(f\"Bounding box: {bbox}\\n\")\n",
    "\n",
    "# Add error handling\n",
    "try:\n",
    "    response = requests.post(url, data={'data': query}, timeout=120)\n",
    "    \n",
    "    print(f\"Response status code: {response.status_code}\")\n",
    "    print(f\"Response content type: {response.headers.get('Content-Type', 'Unknown')}\")\n",
    "    \n",
    "    # Check if response is successful\n",
    "    if response.status_code == 200:\n",
    "        # Print first 500 chars of response to debug\n",
    "        print(f\"Response preview (first 500 chars):\\n{response.text[:500]}\\n\")\n",
    "        \n",
    "        # Try to parse JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Print a summary of the result\n",
    "        print(f\"✓ Successfully received data!\")\n",
    "        print(f\"Number of elements: {len(data.get('elements', []))}\")\n",
    "        \n",
    "        if data.get('elements'):\n",
    "            print(f\"\\nFirst 3 elements:\")\n",
    "            for i, elem in enumerate(data['elements'][:3]):\n",
    "                print(f\"\\nElement {i+1}:\")\n",
    "                print(f\"  Type: {elem.get('type')}\")\n",
    "                print(f\"  ID: {elem.get('id')}\")\n",
    "                print(f\"  Tags: {elem.get('tags', {})}\")\n",
    "    elif response.status_code == 429:\n",
    "        print(\"⚠ Rate limited! Too many requests. Wait a moment and try again.\")\n",
    "    elif response.status_code == 504:\n",
    "        print(\"⚠ Gateway timeout! The query took too long. Try reducing the search area.\")\n",
    "    else:\n",
    "        print(f\"⚠ Error: {response.status_code}\")\n",
    "        print(f\"Response text:\\n{response.text[:1000]}\")\n",
    "        \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"⚠ Request timed out! The server took too long to respond.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"⚠ Request failed: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"⚠ Failed to parse JSON response: {e}\")\n",
    "    print(f\"Response text:\\n{response.text[:1000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953ce1d",
   "metadata": {},
   "source": [
    "## Label Mapping for Air Quality Data\n",
    "Create a mapping of the fetched OSM features to labels relevant for air quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3088ef50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting OSM feature labels...\n",
      "\n",
      "======================================================================\n",
      "OSM FEATURE LABELS EXTRACTION RESULTS\n",
      "======================================================================\n",
      "Total OSM elements found: 22953\n",
      "Unique feature types: 6\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "EXTRACTED FEATURE LABELS BY CATEGORY\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "AMENITY:\n",
      "  Labels: college,fuel\n",
      "  Count of unique values: 2\n",
      "\n",
      "BARRIER:\n",
      "  Labels: gate\n",
      "  Count of unique values: 1\n",
      "\n",
      "BOUNDARY:\n",
      "  Labels: administrative\n",
      "  Count of unique values: 1\n",
      "\n",
      "HIGHWAY:\n",
      "  Labels: residential,service,tertiary,trunk,unclassified\n",
      "  Count of unique values: 5\n",
      "\n",
      "LANDUSE:\n",
      "  Labels: industrial\n",
      "  Count of unique values: 1\n",
      "\n",
      "RAILWAY:\n",
      "  Labels: rail\n",
      "  Count of unique values: 1\n",
      "\n",
      "======================================================================\n",
      "FEATURE DATAFRAME CREATED\n",
      "======================================================================\n",
      "Shape: (1, 12)\n",
      "Columns: 12\n",
      "\n",
      "Columns: ['station_id', 'station_name', 'latitude', 'longitude', 'barrier', 'highway', 'railway', 'landuse', 'amenity', 'boundary', '_total_elements', '_unique_feature_types']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SAMPLE DATA (First Row)\n",
      "----------------------------------------------------------------------\n",
      "station_id          : site_5094\n",
      "station_name        : SIDCO Kurichi, Coimbatore - TNPCB\n",
      "latitude            : 10.942451\n",
      "longitude           : 76.978996\n",
      "barrier             : gate\n",
      "highway             : residential,service,tertiary,trunk,unclassified\n",
      "railway             : rail\n",
      "landuse             : industrial\n",
      "amenity             : college,fuel\n",
      "boundary            : administrative\n",
      "\n",
      "✓ Feature label extraction complete!\n",
      "  Each column contains comma-separated labels for that feature type.\n",
      "  This labeled data can be used for air quality prediction!\n"
     ]
    }
   ],
   "source": [
    "# Extract OSM Feature Labels for ML Training\n",
    "def extract_osm_feature_labels(osm_data):\n",
    "    \"\"\"\n",
    "    Extract OSM features as LABELS (not counts).\n",
    "    Each primary feature key becomes a column with comma-separated values.\n",
    "    \n",
    "    Example output:\n",
    "    - highway: \"trunk,secondary,residential\"\n",
    "    - landuse: \"industrial,commercial\"\n",
    "    - amenity: \"fuel,parking,hospital\"\n",
    "    \n",
    "    Returns:\n",
    "        dict: Feature labels {key: \"value1,value2,value3\"}\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Store unique values for each feature key\n",
    "    feature_values = defaultdict(set)\n",
    "    \n",
    "    # PRIMARY feature keys from OSM Map Features\n",
    "    primary_feature_keys = {\n",
    "        'aerialway', 'aeroway', 'highway', 'railway', 'public_transport',\n",
    "        'landuse', 'natural', 'leisure', 'place', 'building',\n",
    "        'amenity', 'shop', 'tourism', 'office',\n",
    "        'man_made', 'power', 'craft', 'industrial',\n",
    "        'emergency', 'healthcare',\n",
    "        'waterway', 'water', 'geological',\n",
    "        'barrier', 'boundary', 'historic', 'military', 'sport'\n",
    "    }\n",
    "    \n",
    "    # Process OSM elements\n",
    "    elements = osm_data.get('elements', [])\n",
    "    total_elements = len(elements)\n",
    "    \n",
    "    for element in elements:\n",
    "        tags = element.get('tags', {})\n",
    "        \n",
    "        for key, value in tags.items():\n",
    "            # ONLY include primary feature keys\n",
    "            if key not in primary_feature_keys:\n",
    "                continue\n",
    "            \n",
    "            # Skip generic/non-informative values\n",
    "            skip_values = ['yes', 'no', 'unknown', '']\n",
    "            if value in skip_values:\n",
    "                continue\n",
    "            \n",
    "            # Clean the value\n",
    "            safe_value = str(value).replace(' ', '_').replace(':', '_').replace('-', '_').replace(',', '_').replace('/', '_').replace('.', '_')\n",
    "            \n",
    "            # Add to set (automatically handles duplicates)\n",
    "            feature_values[key].add(safe_value)\n",
    "    \n",
    "    # Convert sets to comma-separated strings\n",
    "    feature_labels = {}\n",
    "    for key, values in feature_values.items():\n",
    "        # Sort values for consistency\n",
    "        sorted_values = sorted(list(values))\n",
    "        feature_labels[key] = ','.join(sorted_values)\n",
    "    \n",
    "    # Add metadata\n",
    "    feature_labels['_total_elements'] = total_elements\n",
    "    feature_labels['_unique_feature_types'] = len(feature_values)\n",
    "    \n",
    "    return feature_labels, feature_values\n",
    "\n",
    "\n",
    "# Apply feature extraction to the data we retrieved\n",
    "if 'data' in locals() and data.get('elements'):\n",
    "    print(\"Extracting OSM feature labels...\\n\")\n",
    "    feature_labels, feature_values_dict = extract_osm_feature_labels(data)\n",
    "    \n",
    "    # Display the feature extraction results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"OSM FEATURE LABELS EXTRACTION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total OSM elements found: {feature_labels.get('_total_elements', 0)}\")\n",
    "    print(f\"Unique feature types: {feature_labels.get('_unique_feature_types', 0)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"EXTRACTED FEATURE LABELS BY CATEGORY\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Display features by category\n",
    "    for key in sorted(feature_values_dict.keys()):\n",
    "        values = feature_labels[key]\n",
    "        value_list = values.split(',')\n",
    "        print(f\"\\n{key.upper()}:\")\n",
    "        print(f\"  Labels: {values}\")\n",
    "        print(f\"  Count of unique values: {len(value_list)}\")\n",
    "    \n",
    "    # Create DataFrame for this station\n",
    "    feature_df = pd.DataFrame([feature_labels])\n",
    "    feature_df.insert(0, 'station_id', df_stations.iloc[0]['station_id'])\n",
    "    feature_df.insert(1, 'station_name', df_stations.iloc[0]['station_name'])\n",
    "    feature_df.insert(2, 'latitude', df_stations.iloc[0]['latitude'])\n",
    "    feature_df.insert(3, 'longitude', df_stations.iloc[0]['longitude'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FEATURE DATAFRAME CREATED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Shape: {feature_df.shape}\")\n",
    "    print(f\"Columns: {len(feature_df.columns)}\")\n",
    "    print(f\"\\nColumns: {list(feature_df.columns)}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"SAMPLE DATA (First Row)\")\n",
    "    print(\"-\" * 70)\n",
    "    for col in feature_df.columns:\n",
    "        if not col.startswith('_'):\n",
    "            print(f\"{col:20s}: {feature_df[col].iloc[0]}\")\n",
    "    \n",
    "    print(\"\\n✓ Feature label extraction complete!\")\n",
    "    print(\"  Each column contains comma-separated labels for that feature type.\")\n",
    "    print(\"  This labeled data can be used for air quality prediction!\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No OSM data available. Please run the previous cell to fetch data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac129a0c",
   "metadata": {},
   "source": [
    "## Save Mapped Features to CSV\n",
    "Save the mapped features and labels to a new CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6be239a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature data saved to 'station_osm_features.csv'\n",
      "  - Rows: 1\n",
      "  - Columns: 12\n",
      "\n",
      "This CSV can be used for ML model training to predict air quality based on location features!\n"
     ]
    }
   ],
   "source": [
    "# Save Extracted Features to CSV\n",
    "if 'feature_df' in locals():\n",
    "    output_file = 'station_osm_features.csv'\n",
    "    feature_df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Feature data saved to '{output_file}'\")\n",
    "    print(f\"  - Rows: {len(feature_df)}\")\n",
    "    print(f\"  - Columns: {len(feature_df.columns)}\")\n",
    "    print(f\"\\nThis CSV can be used for ML model training to predict air quality based on location features!\")\n",
    "else:\n",
    "    print(\"⚠ No feature data to save. Please run the feature extraction cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fee23",
   "metadata": {},
   "source": [
    "## Batch Process Multiple Stations\n",
    "Process multiple stations with automatic retry and rate limiting (5 second delay between requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398fe40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing for 25 stations...\n",
      "Delay between requests: 5 seconds\n",
      "======================================================================\n",
      "\n",
      "[1/25] Processing: SIDCO Kurichi, Coimbatore - TNPCB\n",
      "  Location: (10.942451, 76.978996)\n",
      "  Querying Overpass API... (attempt 1/5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Querying Overpass API... (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_count+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     59\u001b[39m         data = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/requests/sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    743\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/requests/models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    900\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/urllib3/response.py:1250\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1234\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1235\u001b[39m \u001b[33;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[32m   1236\u001b[39m \u001b[33;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1247\u001b[39m \u001b[33;03m    'content-encoding' header.\u001b[39;00m\n\u001b[32m   1248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[32m   1253\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp)\n\u001b[32m   1254\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.has_unconsumed_tail)\n\u001b[32m   1256\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/urllib3/response.py:1418\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1416\u001b[39m     chunk = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left == \u001b[32m0\u001b[39m:\n\u001b[32m   1420\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/.venv/lib/python3.13/site-packages/urllib3/response.py:1333\u001b[39m, in \u001b[36mHTTPResponse._update_chunk_length\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1334\u001b[39m line = line.split(\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Batch Process Multiple Stations\n",
    "import time\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "NUM_STATIONS = 25  # Change this to process more stations\n",
    "DELAY_SECONDS = 5  # Delay between requests to avoid rate limiting\n",
    "RETRY_DELAY = 15  # Delay after a failed request before retrying\n",
    "# ===================================\n",
    "\n",
    "print(f\"Starting batch processing for {NUM_STATIONS} stations...\")\n",
    "print(f\"Delay between requests: {DELAY_SECONDS} seconds\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Storage for all station features\n",
    "all_features = []\n",
    "failed_stations = []\n",
    "\n",
    "for idx in range(min(NUM_STATIONS, len(df_stations))):\n",
    "    station = df_stations.iloc[idx]\n",
    "    station_id = station['station_id']\n",
    "    station_name = station['station_name']\n",
    "    lat = float(station['latitude'])\n",
    "    lon = float(station['longitude'])\n",
    "    \n",
    "    print(f\"\\n[{idx+1}/{NUM_STATIONS}] Processing: {station_name}\")\n",
    "    print(f\"  Location: ({lat}, {lon})\")\n",
    "    \n",
    "    # Build Overpass query\n",
    "    delta = 0.005\n",
    "    bbox = f\"{lat-delta},{lon-delta},{lat+delta},{lon+delta}\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    [bbox:{bbox}]\n",
    "    [out:json]\n",
    "    [timeout:90];\n",
    "    (\n",
    "      node({lat-delta},{lon-delta},{lat+delta},{lon+delta});\n",
    "      way({lat-delta},{lon-delta},{lat+delta},{lon+delta});\n",
    "      relation({lat-delta},{lon-delta},{lat+delta},{lon+delta});\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://overpass-api.de/api/interpreter\"\n",
    "    \n",
    "    # Try to fetch data with retry logic\n",
    "    max_retries = 5\n",
    "    retry_count = 0\n",
    "    success = False\n",
    "    \n",
    "    while retry_count < max_retries and not success:\n",
    "        try:\n",
    "            print(f\"  Querying Overpass API... (attempt {retry_count+1}/{max_retries})\")\n",
    "            response = requests.post(url, data={'data': query}, timeout=120)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                elements_count = len(data.get('elements', []))\n",
    "                print(f\"  ✓ Success! Received {elements_count} elements\")\n",
    "                \n",
    "                # Extract feature labels\n",
    "                feature_labels, _ = extract_osm_feature_labels(data)\n",
    "                \n",
    "                # Add station metadata\n",
    "                feature_labels['station_id'] = station_id\n",
    "                feature_labels['station_name'] = station_name\n",
    "                feature_labels['latitude'] = lat\n",
    "                feature_labels['longitude'] = lon\n",
    "                \n",
    "                all_features.append(feature_labels)\n",
    "                success = True\n",
    "                \n",
    "            elif response.status_code == 429:\n",
    "                print(f\"  ⚠ Rate limited! Waiting {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                retry_count += 1\n",
    "                \n",
    "            elif response.status_code == 504:\n",
    "                print(f\"  ⚠ Gateway timeout! Waiting {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "                retry_count += 1\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ⚠ Error {response.status_code}: {response.text[:200]}\")\n",
    "                retry_count += 1\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"  ⚠ Request timed out! Waiting {RETRY_DELAY} seconds...\")\n",
    "            time.sleep(RETRY_DELAY)\n",
    "            retry_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠ Error: {e}\")\n",
    "            retry_count += 1\n",
    "    \n",
    "    if not success:\n",
    "        print(f\"  ✗ Failed after {max_retries} attempts\")\n",
    "        failed_stations.append({'station_id': station_id, 'station_name': station_name})\n",
    "    \n",
    "    # Wait before next request (except for the last one)\n",
    "    if idx < NUM_STATIONS - 1:\n",
    "        print(f\"  Waiting {DELAY_SECONDS} seconds before next request...\")\n",
    "        time.sleep(DELAY_SECONDS)\n",
    "\n",
    "# Create final DataFrame\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BATCH PROCESSING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Successfully processed: {len(all_features)} stations\")\n",
    "print(f\"Failed: {len(failed_stations)} stations\")\n",
    "\n",
    "if all_features:\n",
    "    # Create DataFrame with all features\n",
    "    batch_df = pd.DataFrame(all_features)\n",
    "    \n",
    "    # Reorder columns: station info first, then features\n",
    "    info_cols = ['station_id', 'station_name', 'latitude', 'longitude']\n",
    "    feature_cols = [col for col in batch_df.columns if col not in info_cols]\n",
    "    batch_df = batch_df[info_cols + sorted(feature_cols)]\n",
    "    \n",
    "    print(f\"\\nDataFrame shape: {batch_df.shape}\")\n",
    "    print(f\"Columns: {len(batch_df.columns)}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = f'station_osm_features_batch_{NUM_STATIONS}.csv'\n",
    "    batch_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Saved to '{output_file}'\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"SUMMARY OF EXTRACTED FEATURES\")\n",
    "    print(\"-\" * 70)\n",
    "    print(batch_df.head())\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⚠ No data was successfully extracted!\")\n",
    "\n",
    "if failed_stations:\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"FAILED STATIONS\")\n",
    "    print(\"-\" * 70)\n",
    "    for failed in failed_stations:\n",
    "        print(f\"  - {failed['station_name']} ({failed['station_id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b4bb8",
   "metadata": {},
   "source": [
    "## Unify Station IDs Across Datasets\n",
    "Map station IDs from different sources to create a unified dataset with consistent station identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e5e4de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "======================================================================\n",
      "✓ Loaded stations.csv: 230 stations\n",
      "  Sample IDs: ['AP001', 'AP002', 'AP003']\n",
      "\n",
      "✓ Loaded station_locations.csv: 567 stations\n",
      "  Sample IDs: ['site_5094', 'site_5124', 'site_147']\n",
      "\n",
      "✓ Loaded station_osm_features_batch_230.csv: 230 stations\n",
      "  Sample IDs: ['site_5094', 'site_5124', 'site_147']\n",
      "\n",
      "======================================================================\n",
      "DATASET SUMMARY\n",
      "======================================================================\n",
      "Total unique station names in stations.csv: 230\n",
      "Total unique station names in station_locations.csv: 567\n",
      "Total unique station names in OSM features: 230\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load stations.csv (has state-based IDs like AP001, DL001)\n",
    "df_stations_master = pd.read_csv('stations.csv')\n",
    "print(f\"✓ Loaded stations.csv: {len(df_stations_master)} stations\")\n",
    "print(f\"  Sample IDs: {df_stations_master['StationId'].head(3).tolist()}\")\n",
    "\n",
    "# Load station_locations.csv (has site_XXXX IDs)\n",
    "df_locations = pd.read_csv('station_locations.csv')\n",
    "print(f\"\\n✓ Loaded station_locations.csv: {len(df_locations)} stations\")\n",
    "print(f\"  Sample IDs: {df_locations['station_id'].head(3).tolist()}\")\n",
    "\n",
    "# Load OSM features (has site_XXXX IDs)\n",
    "df_osm = pd.read_csv('station_osm_features_batch_230.csv')\n",
    "print(f\"\\n✓ Loaded station_osm_features_batch_230.csv: {len(df_osm)} stations\")\n",
    "print(f\"  Sample IDs: {df_osm['station_id'].head(3).tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total unique station names in stations.csv: {df_stations_master['StationName'].nunique()}\")\n",
    "print(f\"Total unique station names in station_locations.csv: {df_locations['station_name'].nunique()}\")\n",
    "print(f\"Total unique station names in OSM features: {df_osm['station_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43da8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating unified station ID mapping...\n",
      "======================================================================\n",
      "\n",
      "Mapping station_locations.csv IDs...\n",
      "  Matched: 218/567 stations\n",
      "  Unmatched: 349/567 stations\n",
      "\n",
      "Mapping OSM features IDs...\n",
      "  Matched: 191/230 stations\n",
      "  Unmatched: 39/230 stations\n",
      "\n",
      "======================================================================\n",
      "UNIFIED ID MAPPING COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Example mappings (first 10 matched):\n",
      "----------------------------------------------------------------------\n",
      "  site_5094    → TN005     |  SIDCO Kurichi, Coimbatore - TNPCB\n",
      "  site_5124    → KA012     |  Urban, Chamarajanagar - KSPCB\n",
      "  site_147     → HR026     |  MD University, Rohtak - HSPCB\n",
      "  site_119     → DL034     |  Sirifort, Delhi - CPCB\n",
      "  site_1549    → PB002     |  Hardev Nagar, Bathinda - PPCB\n",
      "  site_5115    → MH014     |  Worli, Mumbai - MPCB\n",
      "  site_288     → TN004     |  Velachery Res. Area, Chennai - CPCB\n",
      "  site_5269    → MP013     |  Deen Dayal Nagar, Sagar - MPPCB\n",
      "  site_5039    → HR021     |  Sector-2 IMT, Manesar - HSPCB\n",
      "  site_5092    → TN002     |  Manali Village, Chennai - TNPCB\n",
      "\n",
      "Unmatched stations in station_locations.csv (keeping original site_XXXX IDs):\n",
      "----------------------------------------------------------------------\n",
      "  site_5468    (no match)  |  IESD Banaras Hindu University, Varanasi - UPPCB\n",
      "  site_5390    (no match)  |  Corporation Ground, Thrissur - Kerala PCB\n",
      "  site_5503    (no match)  |  Mangala, Bilaspur - NTPC\n",
      "  site_5628    (no match)  |  Civil Lines, Bareilly - UPPCB\n",
      "  site_5625    (no match)  |  Chalai Bazaar, Ramanathapuram - TNPCB\n",
      "  site_5395    (no match)  |  Lodhi Road, Delhi - IITM\n",
      "  site_5412    (no match)  |  Kandivali East, Mumbai - MPCB\n",
      "  site_258     (no match)  |  Toll Gate, Tirumala - APPCB\n",
      "  site_271     (no match)  |  Chauhan Colony, Chandrapur - MPCB\n",
      "  site_299     (no match)  |  Womens College_City Center, Durgapur - WBPCB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/xsy51vhx14z013rcw2jjgtlm0000gn/T/ipykernel_33855/3142674048.py:29: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
      "\n",
      "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
      "  df_locations['unified_station_id'].fillna(df_locations['station_id'], inplace=True)\n",
      "/var/folders/9_/xsy51vhx14z013rcw2jjgtlm0000gn/T/ipykernel_33855/3142674048.py:40: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
      "\n",
      "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
      "  df_osm['unified_station_id'].fillna(df_osm['station_id'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Create unified station ID mapping based on station names\n",
    "print(\"Creating unified station ID mapping...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Function to normalize station names for matching\n",
    "def normalize_name(name):\n",
    "    \"\"\"Normalize station name for matching (remove extra spaces, lowercase)\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    return str(name).strip().lower()\n",
    "\n",
    "# Create normalized name columns\n",
    "df_stations_master['normalized_name'] = df_stations_master['StationName'].apply(normalize_name)\n",
    "df_locations['normalized_name'] = df_locations['station_name'].apply(normalize_name)\n",
    "df_osm['normalized_name'] = df_osm['station_name'].apply(normalize_name)\n",
    "\n",
    "# Create mapping from site_XXXX to state-based IDs (using station names as keys)\n",
    "name_to_state_id = dict(zip(df_stations_master['normalized_name'], df_stations_master['StationId']))\n",
    "\n",
    "# Map site_XXXX to state-based IDs for station_locations\n",
    "print(\"\\nMapping station_locations.csv IDs...\")\n",
    "df_locations['unified_station_id'] = df_locations['normalized_name'].map(name_to_state_id)\n",
    "matched_locations = df_locations['unified_station_id'].notna().sum()\n",
    "unmatched_locations = df_locations['unified_station_id'].isna().sum()\n",
    "print(f\"  Matched: {matched_locations}/{len(df_locations)} stations\")\n",
    "print(f\"  Unmatched: {unmatched_locations}/{len(df_locations)} stations\")\n",
    "\n",
    "# For unmatched, keep original site_XXXX ID\n",
    "df_locations['unified_station_id'].fillna(df_locations['station_id'], inplace=True)\n",
    "\n",
    "# Map site_XXXX to state-based IDs for OSM features\n",
    "print(\"\\nMapping OSM features IDs...\")\n",
    "df_osm['unified_station_id'] = df_osm['normalized_name'].map(name_to_state_id)\n",
    "matched_osm = df_osm['unified_station_id'].notna().sum()\n",
    "unmatched_osm = df_osm['unified_station_id'].isna().sum()\n",
    "print(f\"  Matched: {matched_osm}/{len(df_osm)} stations\")\n",
    "print(f\"  Unmatched: {unmatched_osm}/{len(df_osm)} stations\")\n",
    "\n",
    "# For unmatched, keep original site_XXXX ID\n",
    "df_osm['unified_station_id'].fillna(df_osm['station_id'], inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"UNIFIED ID MAPPING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show some examples of the mapping\n",
    "print(\"\\nExample mappings (first 10 matched):\")\n",
    "print(\"-\" * 70)\n",
    "mapping_examples = df_locations[df_locations['normalized_name'].isin(name_to_state_id)].head(10)\n",
    "for _, row in mapping_examples.iterrows():\n",
    "    print(f\"  {row['station_id']:12s} → {row['unified_station_id']:8s}  |  {row['station_name'][:50]}\")\n",
    "\n",
    "# Show unmatched stations\n",
    "if unmatched_locations > 0:\n",
    "    print(f\"\\nUnmatched stations in station_locations.csv (keeping original site_XXXX IDs):\")\n",
    "    print(\"-\" * 70)\n",
    "    unmatched = df_locations[~df_locations['normalized_name'].isin(name_to_state_id)].head(10)\n",
    "    for _, row in unmatched.iterrows():\n",
    "        print(f\"  {row['station_id']:12s} (no match)  |  {row['station_name'][:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63ce4c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating unified datasets...\n",
      "======================================================================\n",
      "✓ Created unified station_locations: 567 stations\n",
      "  Columns: ['station_id', 'original_station_id', 'station_name', 'latitude', 'longitude', 'city', 'state']\n",
      "\n",
      "✓ Created unified OSM features: 230 stations\n",
      "  Columns: 33\n",
      "\n",
      "======================================================================\n",
      "SAMPLE OF UNIFIED DATA\n",
      "======================================================================\n",
      "\n",
      "Unified station_locations (first 5 rows):\n",
      "  station_id original_station_id  \\\n",
      "0      TN005           site_5094   \n",
      "1      KA012           site_5124   \n",
      "2      HR026            site_147   \n",
      "3        NaN           site_5468   \n",
      "4      DL034            site_119   \n",
      "\n",
      "                                      station_name   latitude  longitude  \\\n",
      "0                SIDCO Kurichi, Coimbatore - TNPCB  10.942451  76.978996   \n",
      "1                    Urban, Chamarajanagar - KSPCB  11.553580  76.555210   \n",
      "2                    MD University, Rohtak - HSPCB  28.521230  76.371380   \n",
      "3  IESD Banaras Hindu University, Varanasi - UPPCB  25.262326  82.995408   \n",
      "4                           Sirifort, Delhi - CPCB  28.550425  77.215938   \n",
      "\n",
      "   city  state  \n",
      "0   NaN    NaN  \n",
      "1   NaN    NaN  \n",
      "2   NaN    NaN  \n",
      "3   NaN    NaN  \n",
      "4   NaN    NaN  \n",
      "\n",
      "Unified OSM features (first 3 rows, selected columns):\n",
      "  station_id original_station_id                       station_name  \\\n",
      "0      TN005           site_5094  SIDCO Kurichi, Coimbatore - TNPCB   \n",
      "1      KA012           site_5124      Urban, Chamarajanagar - KSPCB   \n",
      "2      HR026            site_147      MD University, Rohtak - HSPCB   \n",
      "\n",
      "    latitude  longitude  _total_elements  \n",
      "0  10.942451  76.978996            22953  \n",
      "1  11.553580  76.555210            13203  \n",
      "2  28.521230  76.371380              905  \n"
     ]
    }
   ],
   "source": [
    "# Create unified datasets with consistent station IDs\n",
    "print(\"Creating unified datasets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Create unified station_locations with state-based IDs\n",
    "df_locations_unified = df_locations.copy()\n",
    "df_locations_unified['original_station_id'] = df_locations_unified['station_id']\n",
    "df_locations_unified['station_id'] = df_locations_unified['unified_station_id']\n",
    "df_locations_unified = df_locations_unified.drop(['normalized_name', 'unified_station_id'], axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "cols = ['station_id', 'original_station_id', 'station_name', 'latitude', 'longitude', 'city', 'state']\n",
    "df_locations_unified = df_locations_unified[cols]\n",
    "\n",
    "print(f\"✓ Created unified station_locations: {len(df_locations_unified)} stations\")\n",
    "print(f\"  Columns: {list(df_locations_unified.columns)}\")\n",
    "\n",
    "# 2. Create unified OSM features with state-based IDs\n",
    "df_osm_unified = df_osm.copy()\n",
    "df_osm_unified['original_station_id'] = df_osm_unified['station_id']\n",
    "df_osm_unified['station_id'] = df_osm_unified['unified_station_id']\n",
    "df_osm_unified = df_osm_unified.drop(['normalized_name', 'unified_station_id'], axis=1)\n",
    "\n",
    "# Reorder columns: station info first\n",
    "info_cols = ['station_id', 'original_station_id', 'station_name', 'latitude', 'longitude']\n",
    "feature_cols = [col for col in df_osm_unified.columns if col not in info_cols]\n",
    "df_osm_unified = df_osm_unified[info_cols + sorted(feature_cols)]\n",
    "\n",
    "print(f\"\\n✓ Created unified OSM features: {len(df_osm_unified)} stations\")\n",
    "print(f\"  Columns: {len(df_osm_unified.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAMPLE OF UNIFIED DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nUnified station_locations (first 5 rows):\")\n",
    "print(df_locations_unified.head())\n",
    "\n",
    "print(\"\\nUnified OSM features (first 3 rows, selected columns):\")\n",
    "display_cols = ['station_id', 'original_station_id', 'station_name', 'latitude', 'longitude', '_total_elements']\n",
    "print(df_osm_unified[display_cols].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3f6f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving unified datasets...\n",
      "======================================================================\n",
      "✓ Saved unified station locations to 'station_locations_unified.csv'\n",
      "  - Rows: 567\n",
      "  - Columns: 7\n",
      "\n",
      "✓ Saved unified OSM features to 'station_osm_features_unified.csv'\n",
      "  - Rows: 230\n",
      "  - Columns: 33\n",
      "\n",
      "======================================================================\n",
      "UNIFICATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Key changes:\n",
      "  • Station IDs from 'site_XXXX' format mapped to state-based IDs (AP001, DL001, etc.)\n",
      "  • Original site_XXXX IDs preserved in 'original_station_id' column\n",
      "  • Stations not found in stations.csv retain their site_XXXX IDs\n",
      "\n",
      "Unified datasets can now be merged with stations.csv using 'station_id'!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "UNIFIED ID DISTRIBUTION\n",
      "----------------------------------------------------------------------\n",
      "State-based IDs (AP001, DL001, etc.): 191\n",
      "Site-based IDs (site_XXXX): 0\n",
      "Total: 230\n"
     ]
    }
   ],
   "source": [
    "# Save unified datasets to CSV\n",
    "print(\"Saving unified datasets...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save unified station_locations\n",
    "output_file_locations = 'station_locations_unified.csv'\n",
    "df_locations_unified.to_csv(output_file_locations, index=False)\n",
    "print(f\"✓ Saved unified station locations to '{output_file_locations}'\")\n",
    "print(f\"  - Rows: {len(df_locations_unified)}\")\n",
    "print(f\"  - Columns: {len(df_locations_unified.columns)}\")\n",
    "\n",
    "# Save unified OSM features\n",
    "output_file_osm = 'station_osm_features_unified.csv'\n",
    "df_osm_unified.to_csv(output_file_osm, index=False)\n",
    "print(f\"\\n✓ Saved unified OSM features to '{output_file_osm}'\")\n",
    "print(f\"  - Rows: {len(df_osm_unified)}\")\n",
    "print(f\"  - Columns: {len(df_osm_unified.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"UNIFICATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nKey changes:\")\n",
    "print(\"  • Station IDs from 'site_XXXX' format mapped to state-based IDs (AP001, DL001, etc.)\")\n",
    "print(\"  • Original site_XXXX IDs preserved in 'original_station_id' column\")\n",
    "print(\"  • Stations not found in stations.csv retain their site_XXXX IDs\")\n",
    "print(f\"\\nUnified datasets can now be merged with stations.csv using 'station_id'!\")\n",
    "\n",
    "# Show ID distribution\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"UNIFIED ID DISTRIBUTION\")\n",
    "print(\"-\" * 70)\n",
    "state_based_ids = df_osm_unified['station_id'].str.match(r'^[A-Z]{2}\\d{3}$').sum()\n",
    "site_based_ids = df_osm_unified['station_id'].str.match(r'^site_\\d+$').sum()\n",
    "print(f\"State-based IDs (AP001, DL001, etc.): {state_based_ids}\")\n",
    "print(f\"Site-based IDs (site_XXXX): {site_based_ids}\")\n",
    "print(f\"Total: {len(df_osm_unified)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d442b",
   "metadata": {},
   "source": [
    "## Find Unmapped Stations from stations.csv\n",
    "Identify which stations from stations.csv are NOT present in the unified dataset (not yet processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e3607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing stations from stations.csv not yet in unified dataset...\n",
      "======================================================================\n",
      "Total stations in stations.csv: 230\n",
      "Total stations in unified dataset: 200\n",
      "Stations in stations.csv NOT yet in unified dataset: 31\n",
      "\n",
      "======================================================================\n",
      "UNMAPPED STATIONS (31 stations)\n",
      "======================================================================\n",
      "\n",
      "Unmapped Station Details:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Station ID: BR008\n",
      "  Name: Muradpur, Patna - BSPCB\n",
      "  City: Patna\n",
      "  State: Bihar\n",
      "  Status: Active\n",
      "\n",
      "Station ID: DL006\n",
      "  Name: Burari Crossing, Delhi - IMD\n",
      "  City: Delhi\n",
      "  State: Delhi\n",
      "  Status: Inactive\n",
      "\n",
      "Station ID: DL011\n",
      "  Name: East Arjun Nagar, Delhi - CPCB\n",
      "  City: Delhi\n",
      "  State: Delhi\n",
      "  Status: Active\n",
      "\n",
      "Station ID: DL021\n",
      "  Name: NSIT Dwarka, Delhi - CPCB\n",
      "  City: Delhi\n",
      "  State: Delhi\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR002\n",
      "  Name: Arya Nagar, Bahadurgarh - HSPCB\n",
      "  City: Bahadurgarh\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR004\n",
      "  Name: H.B. Colony, Bhiwani - HSPCB\n",
      "  City: Bhiwani\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR005\n",
      "  Name: Municipal Corporation Office, Dharuhera - HSPCB\n",
      "  City: Dharuhera\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR012\n",
      "  Name: Sector-51, Gurugram - HSPCB\n",
      "  City: Gurugram\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR016\n",
      "  Name: Police Lines, Jind - HSPCB\n",
      "  City: Jind\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR017\n",
      "  Name: Rishi Nagar, Kaithal - HSPCB\n",
      "  City: Kaithal\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR018\n",
      "  Name: Sector-12, Karnal - HSPCB\n",
      "  City: Karnal\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR023\n",
      "  Name: Shyam Nagar, Palwal - HSPCB\n",
      "  City: Palwal\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: HR028\n",
      "  Name: Murthal, Sonipat - HSPCB\n",
      "  City: Sonipat\n",
      "  State: Haryana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA002\n",
      "  Name: BTM Layout, Bengaluru - CPCB\n",
      "  City: Bengaluru\n",
      "  State: Karnataka\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA004\n",
      "  Name: Bapuji Nagar, Bengaluru - KSPCB\n",
      "  City: Bengaluru\n",
      "  State: Karnataka\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA006\n",
      "  Name: Hebbal, Bengaluru - KSPCB\n",
      "  City: Bengaluru\n",
      "  State: Karnataka\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA007\n",
      "  Name: Hombegowda Nagar, Bengaluru - KSPCB\n",
      "  City: Bengaluru\n",
      "  State: Karnataka\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA008\n",
      "  Name: Jayanagar 5th Block, Bengaluru - KSPCB\n",
      "  City: Bengaluru\n",
      "  State: Karnataka\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA009\n",
      "  Name: Peenya, Bengaluru - CPCB\n",
      "  City: Bengaluru\n",
      "  State: Karnataka\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA011\n",
      "  Name: Silk Board, Bengaluru - KSPCB\n",
      "  City: Bengaluru\n",
      "  State: Karnataka\n",
      "  Status: Active\n",
      "\n",
      "Station ID: KA013\n",
      "  Name: Chikkaballapur Rural, Chikkaballapur - KSPCB\n",
      "  City: Chikkaballapur\n",
      "  State: Karnataka\n",
      "  Status: nan\n",
      "\n",
      "Station ID: KA016\n",
      "  Name: Lal Bahadur Shastri Nagar, Kalaburagi - KSPCB\n",
      "  City: Kalaburagi\n",
      "  State: Karnataka\n",
      "  Status: nan\n",
      "\n",
      "Station ID: KA018\n",
      "  Name: Vijay Nagar, Ramanagara - KSPCB\n",
      "  City: Ramanagara\n",
      "  State: Karnataka\n",
      "  Status: nan\n",
      "\n",
      "Station ID: KL007\n",
      "  Name: Kariavattom, Thiruvananthapuram - Kerala PCB\n",
      "  City: Thiruvananthapuram\n",
      "  State: Kerala\n",
      "  Status: Active\n",
      "\n",
      "Station ID: MH022\n",
      "  Name: Pimpleshwar Mandir, Thane - MPCB\n",
      "  City: Thane\n",
      "  State: Maharashtra\n",
      "  Status: nan\n",
      "\n",
      "Station ID: ML001\n",
      "  Name: Lumpyngngad, Shillong - Meghalaya PCB\n",
      "  City: Shillong\n",
      "  State: Meghalaya\n",
      "  Status: Active\n",
      "\n",
      "Station ID: MP014\n",
      "  Name: Bandhavgar Colony, Satna - Birla Cement\n",
      "  City: Satna\n",
      "  State: Madhya Pradesh\n",
      "  Status: nan\n",
      "\n",
      "Station ID: TG002\n",
      "  Name: Central University, Hyderabad - TSPCB\n",
      "  City: Hyderabad\n",
      "  State: Telangana\n",
      "  Status: Active\n",
      "\n",
      "Station ID: TN001\n",
      "  Name: Alandur Bus Depot, Chennai - CPCB\n",
      "  City: Chennai\n",
      "  State: Tamil Nadu\n",
      "  Status: Active\n",
      "\n",
      "Station ID: UP012\n",
      "  Name: Central School, Lucknow - CPCB\n",
      "  City: Lucknow\n",
      "  State: Uttar Pradesh\n",
      "  Status: Active\n",
      "\n",
      "Station ID: UP024\n",
      "  Name: Sector-1, Noida - UPPCB\n",
      "  City: Noida\n",
      "  State: Uttar Pradesh\n",
      "  Status: nan\n",
      "\n",
      "======================================================================\n",
      "SUMMARY TABLE OF UNMAPPED STATIONS\n",
      "======================================================================\n",
      "StationId                                     StationName               City          State   Status                                 normalized_name\n",
      "    BR008                         Muradpur, Patna - BSPCB              Patna          Bihar   Active                         muradpur, patna - bspcb\n",
      "    DL006                    Burari Crossing, Delhi - IMD              Delhi          Delhi Inactive                    burari crossing, delhi - imd\n",
      "    DL011                  East Arjun Nagar, Delhi - CPCB              Delhi          Delhi   Active                  east arjun nagar, delhi - cpcb\n",
      "    DL021                       NSIT Dwarka, Delhi - CPCB              Delhi          Delhi   Active                       nsit dwarka, delhi - cpcb\n",
      "    HR002                 Arya Nagar, Bahadurgarh - HSPCB        Bahadurgarh        Haryana   Active                 arya nagar, bahadurgarh - hspcb\n",
      "    HR004                    H.B. Colony, Bhiwani - HSPCB            Bhiwani        Haryana   Active                    h.b. colony, bhiwani - hspcb\n",
      "    HR005 Municipal Corporation Office, Dharuhera - HSPCB          Dharuhera        Haryana   Active municipal corporation office, dharuhera - hspcb\n",
      "    HR012                     Sector-51, Gurugram - HSPCB           Gurugram        Haryana   Active                     sector-51, gurugram - hspcb\n",
      "    HR016                      Police Lines, Jind - HSPCB               Jind        Haryana   Active                      police lines, jind - hspcb\n",
      "    HR017                    Rishi Nagar, Kaithal - HSPCB            Kaithal        Haryana   Active                    rishi nagar, kaithal - hspcb\n",
      "    HR018                       Sector-12, Karnal - HSPCB             Karnal        Haryana   Active                       sector-12, karnal - hspcb\n",
      "    HR023                     Shyam Nagar, Palwal - HSPCB             Palwal        Haryana   Active                     shyam nagar, palwal - hspcb\n",
      "    HR028                        Murthal, Sonipat - HSPCB            Sonipat        Haryana   Active                        murthal, sonipat - hspcb\n",
      "    KA002                    BTM Layout, Bengaluru - CPCB          Bengaluru      Karnataka   Active                    btm layout, bengaluru - cpcb\n",
      "    KA004                 Bapuji Nagar, Bengaluru - KSPCB          Bengaluru      Karnataka   Active                 bapuji nagar, bengaluru - kspcb\n",
      "    KA006                       Hebbal, Bengaluru - KSPCB          Bengaluru      Karnataka   Active                       hebbal, bengaluru - kspcb\n",
      "    KA007             Hombegowda Nagar, Bengaluru - KSPCB          Bengaluru      Karnataka   Active             hombegowda nagar, bengaluru - kspcb\n",
      "    KA008          Jayanagar 5th Block, Bengaluru - KSPCB          Bengaluru      Karnataka   Active          jayanagar 5th block, bengaluru - kspcb\n",
      "    KA009                        Peenya, Bengaluru - CPCB          Bengaluru      Karnataka   Active                        peenya, bengaluru - cpcb\n",
      "    KA011                   Silk Board, Bengaluru - KSPCB          Bengaluru      Karnataka   Active                   silk board, bengaluru - kspcb\n",
      "    KA013    Chikkaballapur Rural, Chikkaballapur - KSPCB     Chikkaballapur      Karnataka      NaN    chikkaballapur rural, chikkaballapur - kspcb\n",
      "    KA016   Lal Bahadur Shastri Nagar, Kalaburagi - KSPCB         Kalaburagi      Karnataka      NaN   lal bahadur shastri nagar, kalaburagi - kspcb\n",
      "    KA018                 Vijay Nagar, Ramanagara - KSPCB         Ramanagara      Karnataka      NaN                 vijay nagar, ramanagara - kspcb\n",
      "    KL007    Kariavattom, Thiruvananthapuram - Kerala PCB Thiruvananthapuram         Kerala   Active    kariavattom, thiruvananthapuram - kerala pcb\n",
      "    MH022                Pimpleshwar Mandir, Thane - MPCB              Thane    Maharashtra      NaN                pimpleshwar mandir, thane - mpcb\n",
      "    ML001           Lumpyngngad, Shillong - Meghalaya PCB           Shillong      Meghalaya   Active           lumpyngngad, shillong - meghalaya pcb\n",
      "    MP014         Bandhavgar Colony, Satna - Birla Cement              Satna Madhya Pradesh      NaN         bandhavgar colony, satna - birla cement\n",
      "    TG002           Central University, Hyderabad - TSPCB          Hyderabad      Telangana   Active           central university, hyderabad - tspcb\n",
      "    TN001               Alandur Bus Depot, Chennai - CPCB            Chennai     Tamil Nadu   Active               alandur bus depot, chennai - cpcb\n",
      "    UP012                  Central School, Lucknow - CPCB            Lucknow  Uttar Pradesh   Active                  central school, lucknow - cpcb\n",
      "    UP024                         Sector-1, Noida - UPPCB              Noida  Uttar Pradesh      NaN                         sector-1, noida - uppcb\n",
      "\n",
      "✓ Saved unmapped stations list to 'unmapped_stations_from_stations_csv.csv'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "UNMAPPED STATIONS BY STATE\n",
      "----------------------------------------------------------------------\n",
      "  Karnataka: 10 stations\n",
      "  Haryana: 9 stations\n",
      "  Delhi: 3 stations\n",
      "  Uttar Pradesh: 2 stations\n",
      "  Bihar: 1 stations\n",
      "  Kerala: 1 stations\n",
      "  Maharashtra: 1 stations\n",
      "  Meghalaya: 1 stations\n",
      "  Madhya Pradesh: 1 stations\n",
      "  Telangana: 1 stations\n",
      "  Tamil Nadu: 1 stations\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "UNMAPPED STATIONS BY STATUS\n",
      "----------------------------------------------------------------------\n",
      "  Active: 24 stations\n",
      "  Inactive: 1 stations\n"
     ]
    }
   ],
   "source": [
    "# Find stations in stations.csv that are NOT in unified dataset\n",
    "print(\"Analyzing stations from stations.csv not yet in unified dataset...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the datasets (if not already loaded)\n",
    "if 'df_stations_master' not in locals():\n",
    "    df_stations_master = pd.read_csv('stations.csv')\n",
    "if 'df_osm_unified' not in locals():\n",
    "    df_osm_unified = pd.read_csv('station_osm_features_unified.csv')\n",
    "\n",
    "df_osm_unified = pd.read_csv('station_osm_features_unified.csv')\n",
    "# Get the station IDs from both datasets\n",
    "stations_csv_ids = set(df_stations_master['StationId'].unique())\n",
    "unified_ids = set(df_osm_unified['station_id'].unique())\n",
    "\n",
    "# Find missing IDs (in stations.csv but NOT in unified)\n",
    "unmapped_ids = stations_csv_ids - unified_ids\n",
    "\n",
    "print(f\"Total stations in stations.csv: {len(stations_csv_ids)}\")\n",
    "print(f\"Total stations in unified dataset: {len(unified_ids)}\")\n",
    "print(f\"Stations in stations.csv NOT yet in unified dataset: {len(unmapped_ids)}\")\n",
    "\n",
    "if unmapped_ids:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"UNMAPPED STATIONS ({len(unmapped_ids)} stations)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get full details of unmapped stations from stations.csv\n",
    "    unmapped_stations = df_stations_master[df_stations_master['StationId'].isin(unmapped_ids)].copy()\n",
    "    \n",
    "    # Sort by StationId\n",
    "    unmapped_stations = unmapped_stations.sort_values('StationId')\n",
    "    \n",
    "    print(\"\\nUnmapped Station Details:\")\n",
    "    print(\"-\" * 70)\n",
    "    for idx, row in unmapped_stations.iterrows():\n",
    "        print(f\"\\nStation ID: {row['StationId']}\")\n",
    "        print(f\"  Name: {row['StationName']}\")\n",
    "        print(f\"  City: {row['City']}\")\n",
    "        print(f\"  State: {row['State']}\")\n",
    "        print(f\"  Status: {row['Status']}\")\n",
    "    \n",
    "    # Create a summary DataFrame\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SUMMARY TABLE OF UNMAPPED STATIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(unmapped_stations.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV for reference\n",
    "    output_file = 'unmapped_stations_from_stations_csv.csv'\n",
    "    unmapped_stations.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Saved unmapped stations list to '{output_file}'\")\n",
    "    \n",
    "    # Analyze by state\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"UNMAPPED STATIONS BY STATE\")\n",
    "    print(\"-\" * 70)\n",
    "    state_counts = unmapped_stations['State'].value_counts()\n",
    "    for state, count in state_counts.items():\n",
    "        print(f\"  {state}: {count} stations\")\n",
    "    \n",
    "    # Analyze by status\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"UNMAPPED STATIONS BY STATUS\")\n",
    "    print(\"-\" * 70)\n",
    "    status_counts = unmapped_stations['Status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        status_label = status if pd.notna(status) and status != '' else 'Unknown/Inactive'\n",
    "        print(f\"  {status_label}: {count} stations\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✓ All stations from stations.csv are present in the unified dataset!\")\n",
    "    print(\"  No unmapped stations found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aebb4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "932dffd3",
   "metadata": {},
   "source": [
    "## Analyze Blank Values in station_osm_features_batch_230.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd9e369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: station_osm_features_batch_230.csv\n",
      "============================================================\n",
      "Total rows: 230\n",
      "Total columns: 32\n",
      "Total cells: 7,360\n",
      "\n",
      "Blank Value Analysis:\n",
      "  NaN/Null values: 3,374\n",
      "  Empty strings: 0\n",
      "  Total blank values: 3,374\n",
      "  Non-blank values: 3,986\n",
      "  Blank percentage: 45.84%\n",
      "\n",
      "============================================================\n",
      "Blank values per column:\n",
      "============================================================\n",
      "military                      : 225 / 230 ( 97.8%)\n",
      "industrial                    : 222 / 230 ( 96.5%)\n",
      "aeroway                       : 219 / 230 ( 95.2%)\n",
      "emergency                     : 217 / 230 ( 94.3%)\n",
      "craft                         : 211 / 230 ( 91.7%)\n",
      "historic                      : 192 / 230 ( 83.5%)\n",
      "sport                         : 155 / 230 ( 67.4%)\n",
      "waterway                      : 147 / 230 ( 63.9%)\n",
      "water                         : 146 / 230 ( 63.5%)\n",
      "power                         : 139 / 230 ( 60.4%)\n",
      "office                        : 135 / 230 ( 58.7%)\n",
      "public_transport              : 133 / 230 ( 57.8%)\n",
      "healthcare                    : 133 / 230 ( 57.8%)\n",
      "man_made                      : 125 / 230 ( 54.3%)\n",
      "tourism                       : 121 / 230 ( 52.6%)\n",
      "railway                       : 120 / 230 ( 52.2%)\n",
      "place                         : 117 / 230 ( 50.9%)\n",
      "shop                          : 113 / 230 ( 49.1%)\n",
      "boundary                      : 112 / 230 ( 48.7%)\n",
      "natural                       :  99 / 230 ( 43.0%)\n",
      "building                      :  86 / 230 ( 37.4%)\n",
      "barrier                       :  77 / 230 ( 33.5%)\n",
      "leisure                       :  60 / 230 ( 26.1%)\n",
      "amenity                       :  36 / 230 ( 15.7%)\n",
      "landuse                       :  31 / 230 ( 13.5%)\n",
      "highway                       :   3 / 230 (  1.3%)\n",
      "latitude                      :   0 / 230 (  0.0%)\n",
      "longitude                     :   0 / 230 (  0.0%)\n",
      "station_name                  :   0 / 230 (  0.0%)\n",
      "_total_elements               :   0 / 230 (  0.0%)\n",
      "_unique_feature_types         :   0 / 230 (  0.0%)\n",
      "station_id                    :   0 / 230 (  0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Load the batch file\n",
    "df_batch = pd.read_csv('station_osm_features_batch_230.csv')\n",
    "\n",
    "# Calculate blank/missing values\n",
    "total_cells = df_batch.size  # Total number of cells in the dataframe\n",
    "blank_values = df_batch.isna().sum().sum()  # Count all NaN/null values\n",
    "empty_strings = (df_batch == '').sum().sum()  # Count empty strings\n",
    "total_blank = blank_values + empty_strings\n",
    "\n",
    "# Calculate statistics\n",
    "non_blank = total_cells - total_blank\n",
    "blank_percentage = (total_blank / total_cells) * 100\n",
    "\n",
    "print(f\"File: station_osm_features_batch_230.csv\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total rows: {len(df_batch)}\")\n",
    "print(f\"Total columns: {len(df_batch.columns)}\")\n",
    "print(f\"Total cells: {total_cells:,}\")\n",
    "print(f\"\\nBlank Value Analysis:\")\n",
    "print(f\"  NaN/Null values: {blank_values:,}\")\n",
    "print(f\"  Empty strings: {empty_strings:,}\")\n",
    "print(f\"  Total blank values: {total_blank:,}\")\n",
    "print(f\"  Non-blank values: {non_blank:,}\")\n",
    "print(f\"  Blank percentage: {blank_percentage:.2f}%\")\n",
    "\n",
    "# Show blank values per column\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Blank values per column:\")\n",
    "print(f\"{'='*60}\")\n",
    "blank_per_column = df_batch.isna().sum() + (df_batch == '').sum()\n",
    "blank_per_column_sorted = blank_per_column.sort_values(ascending=False)\n",
    "\n",
    "for col, count in blank_per_column_sorted.items():\n",
    "    percentage = (count / len(df_batch)) * 100\n",
    "    print(f\"{col:30s}: {count:3d} / {len(df_batch)} ({percentage:5.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
