{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafe064d",
   "metadata": {},
   "source": [
    "# Air Quality Dataset Aggregator\n",
    "\n",
    "This notebook merges air quality data from multiple CSV files, filters by station ID, adds user-defined attributes, and splits the output into multiple files with a maximum of 10,000 rows each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e2c60",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b36225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe9ed5",
   "metadata": {},
   "source": [
    "## Section 2: Define User Input Parameters\n",
    "\n",
    "Enter the station ID and the values for the new columns to be added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the unified OSM features to get station data automatically\n",
    "current_directory = os.getcwd()\n",
    "unified_features_path = os.path.join(current_directory, \"station_osm_features_unified.csv\")\n",
    "\n",
    "print(\"Loading unified OSM features...\")\n",
    "df_unified_features = pd.read_csv(unified_features_path)\n",
    "print(f\"✓ Loaded {len(df_unified_features)} stations from unified features\")\n",
    "\n",
    "# Identify all feature columns (exclude metadata columns)\n",
    "metadata_cols = ['station_id', 'original_station_id', 'station_name', 'latitude', 'longitude', \n",
    "                 '_total_elements', '_unique_feature_types']\n",
    "feature_columns = [col for col in df_unified_features.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"\\nTotal feature columns: {len(feature_columns)}\")\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "\n",
    "print(f\"\\nFirst 5 stations (station info + first few features):\")\n",
    "display_cols = ['station_id', 'station_name'] + feature_columns[:5]\n",
    "print(df_unified_features[display_cols].head())\n",
    "\n",
    "print(\"\\n✓ Ready to process all stations with ALL features automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e13f1d",
   "metadata": {},
   "source": [
    "## Section 3: Load Station Hour Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9903872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load station_hour.csv data\n",
    "station_hour_path = os.path.join(current_directory, \"station_hour.csv\")\n",
    "\n",
    "print(f\"Loading station_hour.csv from: {station_hour_path}\")\n",
    "station_hour_df = pd.read_csv(station_hour_path)\n",
    "\n",
    "print(f\"✓ Station Hour DataFrame shape: {station_hour_df.shape}\")\n",
    "print(f\"  Columns: {list(station_hour_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a79cc",
   "metadata": {},
   "source": [
    "## Section 4: Process All Stations Automatically\n",
    "\n",
    "This section automatically processes each station from the unified features dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all stations automatically\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTOMATED STATION PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if StationId column exists\n",
    "if 'StationId' not in station_hour_df.columns:\n",
    "    print(\"✗ StationId not found in station_hour.csv\")\n",
    "    print(f\"Available columns: {list(station_hour_df.columns)}\")\n",
    "else:\n",
    "    print(\"✓ StationId found in station_hour.csv\")\n",
    "\n",
    "# Get list of stations to process (only those in station_hour data)\n",
    "available_stations = station_hour_df['StationId'].unique()\n",
    "stations_to_process = df_unified_features[df_unified_features['station_id'].isin(available_stations)]\n",
    "\n",
    "print(f\"\\nTotal stations in unified features: {len(df_unified_features)}\")\n",
    "print(f\"Stations with air quality data: {len(stations_to_process)}\")\n",
    "print(f\"Stations to process: {len(stations_to_process)}\")\n",
    "\n",
    "if len(stations_to_process) == 0:\n",
    "    print(\"\\n⚠️ Warning: No stations to process!\")\n",
    "else:\n",
    "    print(f\"\\n✓ Ready to process {len(stations_to_process)} stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c144639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique StationId values in station_hour.csv\n",
    "unique_station_ids = station_hour_df['StationId'].unique()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"UNIQUE STATION IDs IN STATION_HOUR.CSV\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total unique stations: {len(unique_station_ids)}\")\n",
    "print(f\"\\nStation IDs:\")\n",
    "print(sorted(unique_station_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc39eb",
   "metadata": {},
   "source": [
    "## Filter Unified Features for Stations with Air Quality Data\n",
    "\n",
    "Create a filtered version of the unified features containing only the 110 stations that have air quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter unified features to only include stations with air quality data\n",
    "print(\"Filtering unified features for stations with air quality data...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Filter the unified features dataframe\n",
    "df_unified_filtered = df_unified_features[df_unified_features['station_id'].isin(unique_station_ids)].copy()\n",
    "\n",
    "print(f\"Original unified features: {len(df_unified_features)} stations\")\n",
    "print(f\"Filtered unified features: {len(df_unified_filtered)} stations\")\n",
    "print(f\"Stations excluded: {len(df_unified_features) - len(df_unified_filtered)} stations\")\n",
    "\n",
    "# Save the filtered dataset\n",
    "output_file = 'station_osm_features_filtered_110.csv'\n",
    "df_unified_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved filtered dataset to '{output_file}'\")\n",
    "print(f\"  - Rows: {len(df_unified_filtered)}\")\n",
    "print(f\"  - Columns: {len(df_unified_filtered.columns)}\")\n",
    "\n",
    "# Show some statistics\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"FILTERED DATASET SUMMARY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\nFirst 5 stations in filtered dataset:\")\n",
    "display_cols = ['station_id', 'station_name', 'latitude', 'longitude']\n",
    "print(df_unified_filtered[display_cols].head())\n",
    "\n",
    "print(f\"\\nAll {len(df_unified_filtered)} filtered station IDs:\")\n",
    "print(sorted(df_unified_filtered['station_id'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d60cd3",
   "metadata": {},
   "source": [
    "## Section 5: Process Each Station with Features\n",
    "\n",
    "This section loops through each station, filters the data, adds OSM features, and saves to chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da081e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each station and process\n",
    "chunk_size = 10000\n",
    "total_stations = len(stations_to_process)\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "total_files_created = 0\n",
    "\n",
    "print(f\"Starting automated processing of {total_stations} stations...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, station_row in stations_to_process.iterrows():\n",
    "    station_id = station_row['station_id']\n",
    "    station_name = station_row['station_name']\n",
    "    \n",
    "    print(f\"\\n[{processed_count + 1}/{total_stations}] Processing: {station_id} - {station_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Filter station_hour data for this station\n",
    "        filtered_df = station_hour_df[station_hour_df['StationId'] == station_id].copy()\n",
    "        \n",
    "        if filtered_df.shape[0] == 0:\n",
    "            print(f\"  ⚠️ No data found for {station_id}, skipping...\")\n",
    "            failed_count += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Found {filtered_df.shape[0]} rows\")\n",
    "        \n",
    "        # Add ALL feature columns from unified dataset\n",
    "        for feature_col in feature_columns:\n",
    "            # Get the feature value for this station (handle NaN values)\n",
    "            feature_value = station_row[feature_col]\n",
    "            if pd.isna(feature_value):\n",
    "                feature_value = \"\"  # Empty string for missing features\n",
    "            filtered_df[feature_col] = feature_value\n",
    "        \n",
    "        print(f\"  Added {len(feature_columns)} feature columns\")\n",
    "        \n",
    "        # Create output folder for this station\n",
    "        output_folder = os.path.join(current_directory, \"output\", station_id)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Split into chunks and save\n",
    "        total_rows = filtered_df.shape[0]\n",
    "        num_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, total_rows)\n",
    "            chunk_df = filtered_df.iloc[start_idx:end_idx]\n",
    "            \n",
    "            file_number = i + 1\n",
    "            filename = f\"{station_id}_chunk_{file_number}.csv\"\n",
    "            filepath = os.path.join(output_folder, filename)\n",
    "            \n",
    "            chunk_df.to_csv(filepath, index=False)\n",
    "            total_files_created += 1\n",
    "        \n",
    "        print(f\"  ✓ Saved {num_chunks} chunk file(s) to output/{station_id}/\")\n",
    "        processed_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error processing {station_id}: {e}\")\n",
    "        failed_count += 1\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total stations processed: {processed_count}/{total_stations}\")\n",
    "print(f\"Failed stations: {failed_count}\")\n",
    "print(f\"Total files created: {total_files_created}\")\n",
    "print(f\"\\n✓ All output files saved in 'output/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf1d16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This automated notebook:\n",
    "1. ✓ Loads the unified OSM features dataset\n",
    "2. ✓ Automatically identifies ALL feature columns (not just 6)\n",
    "3. ✓ Loads station_hour.csv with air quality data\n",
    "4. ✓ Processes each station automatically:\n",
    "   - Filters air quality data by station ID\n",
    "   - Adds ALL OSM feature columns from unified dataset\n",
    "   - Splits data into chunks of 10,000 rows\n",
    "   - Saves chunks to `output/[StationID]/` folder\n",
    "5. ✓ Provides detailed progress and summary statistics\n",
    "\n",
    "All output files are organized in the `output/` folder by station ID!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
