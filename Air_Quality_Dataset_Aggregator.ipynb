{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafe064d",
   "metadata": {},
   "source": [
    "# Air Quality Dataset Aggregator\n",
    "\n",
    "This notebook merges air quality data from multiple CSV files, filters by station ID, adds user-defined attributes, and splits the output into multiple files with a maximum of 10,000 rows each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e2c60",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b36225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe9ed5",
   "metadata": {},
   "source": [
    "## Section 2: Define User Input Parameters\n",
    "\n",
    "Enter the station ID and the values for the new columns to be added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fb8843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Parameters:\n",
      "Station ID: AP001\n",
      "Boundary: value1\n",
      "Building: value2\n",
      "Geological: value3\n",
      "Highway: value4\n",
      "Landuse: value5\n",
      "Natural: value6\n"
     ]
    }
   ],
   "source": [
    "# User Input Parameters\n",
    "station_id = \"AP001\"  # Change this to your desired station ID\n",
    "boundary = \"value1\"  # Add your boundary value here\n",
    "building = \"value2\"  # Add your building value here\n",
    "geological = \"value3\"  # Add your geological value here\n",
    "highway = \"value4\"  # Add your highway value here\n",
    "landuse = \"value5\"  # Add your landuse value here\n",
    "natural = \"value6\"  # Add your natural value here\n",
    "\n",
    "print(\"Input Parameters:\")\n",
    "print(f\"Station ID: {station_id}\")\n",
    "print(f\"Boundary: {boundary}\")\n",
    "print(f\"Building: {building}\")\n",
    "print(f\"Geological: {geological}\")\n",
    "print(f\"Highway: {highway}\")\n",
    "print(f\"Landuse: {landuse}\")\n",
    "print(f\"Natural: {natural}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e13f1d",
   "metadata": {},
   "source": [
    "## Section 3: Load CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9903872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV files from: /Users/likhithkanigolla/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT\n",
      "Station Hour CSV path: /Users/likhithkanigolla/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/station_hour.csv\n",
      "Stations CSV path: /Users/likhithkanigolla/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/stations.csv\n",
      "\n",
      "Station Hour DataFrame shape: (2589083, 16)\n",
      "Stations DataFrame shape: (230, 5)\n",
      "\n",
      "Station Hour DataFrame columns: ['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket']\n",
      "Stations DataFrame columns: ['StationId', 'StationName', 'City', 'State', 'Status']\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the CSV files\n",
    "current_directory = os.getcwd()\n",
    "station_hour_path = os.path.join(current_directory, \"station_hour.csv\")\n",
    "stations_path = os.path.join(current_directory, \"stations.csv\")\n",
    "\n",
    "print(f\"Loading CSV files from: {current_directory}\")\n",
    "print(f\"Station Hour CSV path: {station_hour_path}\")\n",
    "print(f\"Stations CSV path: {stations_path}\")\n",
    "\n",
    "# Load the CSV files\n",
    "station_hour_df = pd.read_csv(station_hour_path)\n",
    "stations_df = pd.read_csv(stations_path)\n",
    "\n",
    "print(f\"\\nStation Hour DataFrame shape: {station_hour_df.shape}\")\n",
    "print(f\"Stations DataFrame shape: {stations_df.shape}\")\n",
    "print(f\"\\nStation Hour DataFrame columns: {list(station_hour_df.columns)}\")\n",
    "print(f\"Stations DataFrame columns: {list(stations_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a79cc",
   "metadata": {},
   "source": [
    "## Section 4: Merge and Filter Data by StationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773d6763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for StationId column in both files...\n",
      "✓ StationId found in station_hour.csv\n",
      "✓ StationId found in stations.csv\n",
      "\n",
      "Merged DataFrame shape: (2589083, 20)\n",
      "Filtered DataFrame shape (for station AP001): (22784, 20)\n",
      "✓ Found 22784 rows for station AP001\n"
     ]
    }
   ],
   "source": [
    "# Merge the DataFrames on StationId\n",
    "# First, check if 'StationId' column exists in both DataFrames\n",
    "print(\"Checking for StationId column in both files...\")\n",
    "\n",
    "if 'StationId' in station_hour_df.columns:\n",
    "    print(\"✓ StationId found in station_hour.csv\")\n",
    "else:\n",
    "    print(\"✗ StationId not found in station_hour.csv\")\n",
    "    print(f\"Available columns: {list(station_hour_df.columns)}\")\n",
    "\n",
    "if 'StationId' in stations_df.columns:\n",
    "    print(\"✓ StationId found in stations.csv\")\n",
    "else:\n",
    "    print(\"✗ StationId not found in stations.csv\")\n",
    "    print(f\"Available columns: {list(stations_df.columns)}\")\n",
    "\n",
    "# Merge the two DataFrames on StationId\n",
    "merged_df = station_hour_df.merge(stations_df, on='StationId', how='inner')\n",
    "print(f\"\\nMerged DataFrame shape: {merged_df.shape}\")\n",
    "\n",
    "# Filter by the specified station ID\n",
    "filtered_df = merged_df[merged_df['StationId'] == station_id].copy()\n",
    "print(f\"Filtered DataFrame shape (for station {station_id}): {filtered_df.shape}\")\n",
    "\n",
    "if filtered_df.shape[0] == 0:\n",
    "    print(f\"⚠️ Warning: No data found for station ID {station_id}\")\n",
    "else:\n",
    "    print(f\"✓ Found {filtered_df.shape[0]} rows for station {station_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d60cd3",
   "metadata": {},
   "source": [
    "## Section 5: Add User-Defined Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da081e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-defined columns added:\n",
      "- boundary: value1\n",
      "- building: value2\n",
      "- geological: value3\n",
      "- highway: value4\n",
      "- landuse: value5\n",
      "- natural: value6\n",
      "\n",
      "Updated DataFrame shape: (22784, 26)\n",
      "Updated DataFrame columns: ['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket', 'StationName', 'City', 'State', 'Status', 'boundary', 'building', 'geological', 'highway', 'landuse', 'natural']\n"
     ]
    }
   ],
   "source": [
    "# Add the user-defined columns to the filtered DataFrame\n",
    "filtered_df['boundary'] = boundary\n",
    "filtered_df['building'] = building\n",
    "filtered_df['geological'] = geological\n",
    "filtered_df['highway'] = highway\n",
    "filtered_df['landuse'] = landuse\n",
    "filtered_df['natural'] = natural\n",
    "\n",
    "print(\"User-defined columns added:\")\n",
    "print(f\"- boundary: {boundary}\")\n",
    "print(f\"- building: {building}\")\n",
    "print(f\"- geological: {geological}\")\n",
    "print(f\"- highway: {highway}\")\n",
    "print(f\"- landuse: {landuse}\")\n",
    "print(f\"- natural: {natural}\")\n",
    "print(f\"\\nUpdated DataFrame shape: {filtered_df.shape}\")\n",
    "print(f\"Updated DataFrame columns: {list(filtered_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcc05a",
   "metadata": {},
   "source": [
    "## Section 6: Split Data into Chunks and Save to Output Files\n",
    "\n",
    "This section splits the data into chunks of maximum 10,000 rows and saves each chunk as a separate CSV file in a folder named after the station ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00dc60f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder created: /Users/likhithkanigolla/IIITH/code-files/Digital-Twin/Air-Quality-Dataset-Seggregator-DT/AP001\n",
      "\n",
      "Total rows: 22784\n",
      "Chunk size: 10000\n",
      "Number of chunks: 3\n",
      "Saved: AP001_chunk_1.csv (10000 rows)\n",
      "Saved: AP001_chunk_2.csv (10000 rows)\n",
      "Saved: AP001_chunk_3.csv (2784 rows)\n",
      "\n",
      "✓ Processing complete!\n",
      "\n",
      "Output files created in folder 'AP001':\n",
      "  1. AP001_chunk_1.csv\n",
      "  2. AP001_chunk_2.csv\n",
      "  3. AP001_chunk_3.csv\n"
     ]
    }
   ],
   "source": [
    "# Create output folder named after the station ID\n",
    "output_folder = os.path.join(current_directory, station_id)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"Output folder created: {output_folder}\")\n",
    "\n",
    "# Parameters for chunking\n",
    "chunk_size = 10000\n",
    "total_rows = filtered_df.shape[0]\n",
    "num_chunks = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "\n",
    "print(f\"\\nTotal rows: {total_rows}\")\n",
    "print(f\"Chunk size: {chunk_size}\")\n",
    "print(f\"Number of chunks: {num_chunks}\")\n",
    "\n",
    "# Split the data into chunks and save each chunk as a CSV file\n",
    "output_files = []\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, total_rows)\n",
    "    chunk_df = filtered_df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Create filename with chunk number\n",
    "    file_number = i + 1\n",
    "    filename = f\"{station_id}_chunk_{file_number}.csv\"\n",
    "    filepath = os.path.join(output_folder, filename)\n",
    "    \n",
    "    # Save the chunk to CSV\n",
    "    chunk_df.to_csv(filepath, index=False)\n",
    "    output_files.append(filename)\n",
    "    \n",
    "    print(f\"Saved: {filename} ({chunk_df.shape[0]} rows)\")\n",
    "\n",
    "print(f\"\\n✓ Processing complete!\")\n",
    "print(f\"\\nOutput files created in folder '{station_id}':\")\n",
    "for i, file in enumerate(output_files, 1):\n",
    "    print(f\"  {i}. {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf1d16",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The notebook has successfully:\n",
    "1. ✓ Loaded CSV files from the workspace\n",
    "2. ✓ Merged data from station_hour.csv and stations.csv\n",
    "3. ✓ Filtered the data by station ID\n",
    "4. ✓ Added user-defined columns (boundary, building, geological, highway, landuse, natural)\n",
    "5. ✓ Split the data into chunks of max 10,000 rows\n",
    "6. ✓ Saved each chunk as a separate CSV file in a folder named after the station ID\n",
    "\n",
    "The output files are now ready for use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
